Fri 21 Feb 2025 11:25:51 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:25:51 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:34:43 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:34:43 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:36:24 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:36:24 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:39:46 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:39:46 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:41:31 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:41:31 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data/data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:41:37 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:41:37 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:42:28 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:42:28 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:42:28 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp']
Fri 21 Feb 2025 11:42:28 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 11:42:28 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 11:42:28 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 11:42:38 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:42:38 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:42:38 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 11:42:38 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 11:42:38 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 11:42:38 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 11:44:28 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:44:28 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:44:28 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 11:44:28 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 11:44:28 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 11:44:28 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 11:44:28 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 11:46:52 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:46:52 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:46:52 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 11:46:52 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 11:46:52 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 11:46:52 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 11:46:52 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 11:47:55 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:47:55 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:47:55 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 11:47:55 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 11:47:55 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 11:47:55 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 11:47:55 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 11:48:51 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:48:51 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:48:51 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 11:48:51 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 11:48:51 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 11:48:51 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 11:48:51 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 11:49:55 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:49:55 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:49:55 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 11:49:55 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 11:49:55 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 11:49:55 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 11:49:55 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 11:56:11 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:56:11 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:56:11 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 11:56:11 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 11:56:11 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 11:56:11 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 11:56:11 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 11:57:23 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 11:57:23 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = False
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 11:57:23 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 11:57:23 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 11:57:23 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 11:57:23 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 11:57:23 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 12:04:16 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 12:04:16 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 12:04:16 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 12:04:16 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 12:04:16 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 12:04:16 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 12:04:16 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 12:04:29 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 12:04:29 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 12:04:29 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 12:04:29 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 12:04:29 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 12:04:29 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 12:04:29 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 12:04:38 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 12:04:38 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 2020
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 300
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}
repeatable = False
metrics = ['AUC', 'LogLoss']
topk = [10]
valid_metric = AUC
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
embedding_size = 10
mlp_hidden_size = [64, 64, 64]
dropout_prob = 0.0
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.CONTEXT
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.VALUE
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}


Fri 21 Feb 2025 12:04:38 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 12:04:38 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 12:04:38 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': None, 'mode': {'valid': 'labeled', 'test': 'labeled'}}]
Fri 21 Feb 2025 12:04:38 INFO  NFM(
  (token_embedding_table): FMEmbedding(
    (embedding): Embedding(1067, 10)
  )
  (first_order_linear): FMFirstOrderLinear(
    (token_embedding_table): FMEmbedding(
      (embedding): Embedding(1067, 1)
    )
  )
  (fm): BaseFactorizationMachine()
  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=64, bias=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Sigmoid()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=64, out_features=64, bias=True)
      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Sigmoid()
      (8): Dropout(p=0.0, inplace=False)
      (9): Linear(in_features=64, out_features=64, bias=True)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Sigmoid()
    )
  )
  (predict_layer): Linear(in_features=64, out_features=1, bias=False)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 21230
Fri 21 Feb 2025 12:04:38 INFO  FLOPs: 9726.0
Fri 21 Feb 2025 12:08:23 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 12:08:23 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = MRR@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
mf_embedding_size = 64
mlp_embedding_size = 64
mlp_hidden_size = [128, 64]
dropout_prob = 0.1
mf_train = True
mlp_train = True
use_pretrain = False
mf_pretrain_path = None
mlp_pretrain_path = None
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.GENERAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


Fri 21 Feb 2025 12:08:23 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 12:08:24 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 12:08:24 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
Fri 21 Feb 2025 12:08:24 INFO  NeuMF(
  (user_mf_embedding): Embedding(982, 64)
  (item_mf_embedding): Embedding(85, 64)
  (user_mlp_embedding): Embedding(982, 64)
  (item_mlp_embedding): Embedding(85, 64)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=128, out_features=64, bias=True)
      (5): ReLU()
    )
  )
  (predict_layer): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 161473
Fri 21 Feb 2025 12:08:24 INFO  FLOPs: 24960.0
Fri 21 Feb 2025 12:08:24 INFO  epoch 0 training [time: 0.61s, train loss: 2.7709]
Fri 21 Feb 2025 12:08:24 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:25 INFO  epoch 1 training [time: 0.46s, train loss: 2.7649]
Fri 21 Feb 2025 12:08:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:25 INFO  epoch 2 training [time: 0.32s, train loss: 2.7500]
Fri 21 Feb 2025 12:08:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:26 INFO  epoch 3 training [time: 0.54s, train loss: 2.7209]
Fri 21 Feb 2025 12:08:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:26 INFO  epoch 4 training [time: 0.29s, train loss: 2.6628]
Fri 21 Feb 2025 12:08:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:26 INFO  epoch 5 training [time: 0.43s, train loss: 2.5754]
Fri 21 Feb 2025 12:08:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:27 INFO  epoch 6 training [time: 0.35s, train loss: 2.4645]
Fri 21 Feb 2025 12:08:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:27 INFO  epoch 7 training [time: 0.51s, train loss: 2.3115]
Fri 21 Feb 2025 12:08:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:28 INFO  epoch 8 training [time: 0.39s, train loss: 2.1386]
Fri 21 Feb 2025 12:08:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:28 INFO  epoch 9 training [time: 0.44s, train loss: 1.8873]
Fri 21 Feb 2025 12:08:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:29 INFO  epoch 10 training [time: 0.35s, train loss: 1.5939]
Fri 21 Feb 2025 12:08:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:29 INFO  epoch 11 training [time: 0.44s, train loss: 1.2955]
Fri 21 Feb 2025 12:08:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:29 INFO  epoch 12 training [time: 0.33s, train loss: 1.0705]
Fri 21 Feb 2025 12:08:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:08:30 INFO  epoch 13 training [time: 0.53s, train loss: 0.9338]
Fri 21 Feb 2025 12:08:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-08-24.pth
Fri 21 Feb 2025 12:35:05 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 12:35:05 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = MRR@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
mf_embedding_size = 64
mlp_embedding_size = 64
mlp_hidden_size = [128, 64]
dropout_prob = 0.1
mf_train = True
mlp_train = True
use_pretrain = False
mf_pretrain_path = None
mlp_pretrain_path = None
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.GENERAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


Fri 21 Feb 2025 12:35:05 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 12:35:05 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 12:35:05 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
Fri 21 Feb 2025 12:35:05 INFO  NeuMF(
  (user_mf_embedding): Embedding(982, 64)
  (item_mf_embedding): Embedding(85, 64)
  (user_mlp_embedding): Embedding(982, 64)
  (item_mlp_embedding): Embedding(85, 64)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=128, out_features=64, bias=True)
      (5): ReLU()
    )
  )
  (predict_layer): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 161473
Fri 21 Feb 2025 12:35:05 INFO  FLOPs: 24960.0
Fri 21 Feb 2025 12:35:06 INFO  epoch 0 training [time: 0.40s, train loss: 2.7709]
Fri 21 Feb 2025 12:35:06 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:06 INFO  epoch 1 training [time: 0.37s, train loss: 2.7649]
Fri 21 Feb 2025 12:35:06 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:06 INFO  epoch 2 training [time: 0.47s, train loss: 2.7500]
Fri 21 Feb 2025 12:35:06 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:07 INFO  epoch 3 training [time: 0.60s, train loss: 2.7209]
Fri 21 Feb 2025 12:35:07 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:07 INFO  epoch 4 training [time: 0.34s, train loss: 2.6628]
Fri 21 Feb 2025 12:35:07 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:08 INFO  epoch 5 training [time: 0.48s, train loss: 2.5754]
Fri 21 Feb 2025 12:35:08 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:08 INFO  epoch 6 training [time: 0.44s, train loss: 2.4645]
Fri 21 Feb 2025 12:35:08 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:09 INFO  epoch 7 training [time: 0.39s, train loss: 2.3115]
Fri 21 Feb 2025 12:35:09 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:09 INFO  epoch 8 training [time: 0.48s, train loss: 2.1386]
Fri 21 Feb 2025 12:35:09 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:10 INFO  epoch 9 training [time: 0.54s, train loss: 1.8873]
Fri 21 Feb 2025 12:35:10 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:11 INFO  epoch 10 training [time: 0.58s, train loss: 1.5939]
Fri 21 Feb 2025 12:35:11 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:11 INFO  epoch 11 training [time: 0.45s, train loss: 1.2955]
Fri 21 Feb 2025 12:35:11 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:11 INFO  epoch 12 training [time: 0.38s, train loss: 1.0705]
Fri 21 Feb 2025 12:35:11 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:12 INFO  epoch 13 training [time: 0.57s, train loss: 0.9338]
Fri 21 Feb 2025 12:35:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:12 INFO  epoch 14 training [time: 0.32s, train loss: 0.8442]
Fri 21 Feb 2025 12:35:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:13 INFO  epoch 15 training [time: 0.34s, train loss: 0.7504]
Fri 21 Feb 2025 12:35:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:13 INFO  epoch 16 training [time: 0.42s, train loss: 0.7390]
Fri 21 Feb 2025 12:35:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:14 INFO  epoch 17 training [time: 0.39s, train loss: 0.6031]
Fri 21 Feb 2025 12:35:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:14 INFO  epoch 18 training [time: 0.41s, train loss: 0.7931]
Fri 21 Feb 2025 12:35:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:14 INFO  epoch 19 training [time: 0.34s, train loss: 0.5367]
Fri 21 Feb 2025 12:35:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:15 INFO  epoch 20 training [time: 0.72s, train loss: 0.5918]
Fri 21 Feb 2025 12:35:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:16 INFO  epoch 21 training [time: 0.41s, train loss: 0.4985]
Fri 21 Feb 2025 12:35:16 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:16 INFO  epoch 22 training [time: 0.62s, train loss: 0.5018]
Fri 21 Feb 2025 12:35:16 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:17 INFO  epoch 23 training [time: 0.46s, train loss: 0.5896]
Fri 21 Feb 2025 12:35:17 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:17 INFO  epoch 24 training [time: 0.69s, train loss: 0.5333]
Fri 21 Feb 2025 12:35:17 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:18 INFO  epoch 25 training [time: 0.53s, train loss: 0.4834]
Fri 21 Feb 2025 12:35:18 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:18 INFO  epoch 26 training [time: 0.50s, train loss: 0.4815]
Fri 21 Feb 2025 12:35:19 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:19 INFO  epoch 27 training [time: 0.48s, train loss: 0.5282]
Fri 21 Feb 2025 12:35:19 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:20 INFO  epoch 28 training [time: 0.58s, train loss: 0.4716]
Fri 21 Feb 2025 12:35:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:20 INFO  epoch 29 training [time: 0.36s, train loss: 0.5044]
Fri 21 Feb 2025 12:35:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:20 INFO  epoch 30 training [time: 0.34s, train loss: 0.4346]
Fri 21 Feb 2025 12:35:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:21 INFO  epoch 31 training [time: 0.49s, train loss: 0.4846]
Fri 21 Feb 2025 12:35:21 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:21 INFO  epoch 32 training [time: 0.33s, train loss: 0.3593]
Fri 21 Feb 2025 12:35:21 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:22 INFO  epoch 33 training [time: 0.56s, train loss: 0.4492]
Fri 21 Feb 2025 12:35:22 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:22 INFO  epoch 34 training [time: 0.30s, train loss: 0.4701]
Fri 21 Feb 2025 12:35:22 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:23 INFO  epoch 35 training [time: 0.51s, train loss: 0.4505]
Fri 21 Feb 2025 12:35:23 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:23 INFO  epoch 36 training [time: 0.47s, train loss: 0.5018]
Fri 21 Feb 2025 12:35:23 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:24 INFO  epoch 37 training [time: 0.37s, train loss: 0.5086]
Fri 21 Feb 2025 12:35:24 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:24 INFO  epoch 38 training [time: 0.33s, train loss: 0.3964]
Fri 21 Feb 2025 12:35:24 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:24 INFO  epoch 39 training [time: 0.49s, train loss: 0.4194]
Fri 21 Feb 2025 12:35:24 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:25 INFO  epoch 40 training [time: 0.26s, train loss: 0.4199]
Fri 21 Feb 2025 12:35:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:25 INFO  epoch 41 training [time: 0.47s, train loss: 0.3737]
Fri 21 Feb 2025 12:35:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:26 INFO  epoch 42 training [time: 0.36s, train loss: 0.3773]
Fri 21 Feb 2025 12:35:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:26 INFO  epoch 43 training [time: 0.50s, train loss: 0.4120]
Fri 21 Feb 2025 12:35:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:27 INFO  epoch 44 training [time: 0.38s, train loss: 0.3731]
Fri 21 Feb 2025 12:35:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:27 INFO  epoch 45 training [time: 0.46s, train loss: 0.3781]
Fri 21 Feb 2025 12:35:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:27 INFO  epoch 46 training [time: 0.25s, train loss: 0.3543]
Fri 21 Feb 2025 12:35:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:28 INFO  epoch 47 training [time: 0.37s, train loss: 0.3406]
Fri 21 Feb 2025 12:35:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:28 INFO  epoch 48 training [time: 0.54s, train loss: 0.3259]
Fri 21 Feb 2025 12:35:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:29 INFO  epoch 49 training [time: 0.34s, train loss: 0.4157]
Fri 21 Feb 2025 12:35:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:29 INFO  epoch 50 training [time: 0.29s, train loss: 0.3342]
Fri 21 Feb 2025 12:35:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:29 INFO  epoch 51 training [time: 0.38s, train loss: 0.3787]
Fri 21 Feb 2025 12:35:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:30 INFO  epoch 52 training [time: 0.37s, train loss: 0.3343]
Fri 21 Feb 2025 12:35:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:30 INFO  epoch 53 training [time: 0.39s, train loss: 0.3083]
Fri 21 Feb 2025 12:35:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:30 INFO  epoch 54 training [time: 0.27s, train loss: 0.3006]
Fri 21 Feb 2025 12:35:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:31 INFO  epoch 55 training [time: 0.38s, train loss: 0.3132]
Fri 21 Feb 2025 12:35:31 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:31 INFO  epoch 56 training [time: 0.42s, train loss: 0.3101]
Fri 21 Feb 2025 12:35:31 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:32 INFO  epoch 57 training [time: 0.54s, train loss: 0.3171]
Fri 21 Feb 2025 12:35:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:32 INFO  epoch 58 training [time: 0.44s, train loss: 0.3510]
Fri 21 Feb 2025 12:35:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:33 INFO  epoch 59 training [time: 0.45s, train loss: 0.3106]
Fri 21 Feb 2025 12:35:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:33 INFO  epoch 60 training [time: 0.38s, train loss: 0.2966]
Fri 21 Feb 2025 12:35:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:34 INFO  epoch 61 training [time: 0.40s, train loss: 0.3329]
Fri 21 Feb 2025 12:35:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:34 INFO  epoch 62 training [time: 0.39s, train loss: 0.3636]
Fri 21 Feb 2025 12:35:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:35 INFO  epoch 63 training [time: 0.38s, train loss: 0.2938]
Fri 21 Feb 2025 12:35:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:35 INFO  epoch 64 training [time: 0.44s, train loss: 0.2787]
Fri 21 Feb 2025 12:35:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:35 INFO  epoch 65 training [time: 0.32s, train loss: 0.3149]
Fri 21 Feb 2025 12:35:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:36 INFO  epoch 66 training [time: 0.33s, train loss: 0.2468]
Fri 21 Feb 2025 12:35:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:36 INFO  epoch 67 training [time: 0.36s, train loss: 0.2614]
Fri 21 Feb 2025 12:35:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:36 INFO  epoch 68 training [time: 0.35s, train loss: 0.2680]
Fri 21 Feb 2025 12:35:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:37 INFO  epoch 69 training [time: 0.37s, train loss: 0.2756]
Fri 21 Feb 2025 12:35:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:37 INFO  epoch 70 training [time: 0.42s, train loss: 0.2147]
Fri 21 Feb 2025 12:35:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:38 INFO  epoch 71 training [time: 0.61s, train loss: 0.2777]
Fri 21 Feb 2025 12:35:38 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:38 INFO  epoch 72 training [time: 0.35s, train loss: 0.2498]
Fri 21 Feb 2025 12:35:38 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:39 INFO  epoch 73 training [time: 0.57s, train loss: 0.2464]
Fri 21 Feb 2025 12:35:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:39 INFO  epoch 74 training [time: 0.39s, train loss: 0.2457]
Fri 21 Feb 2025 12:35:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:40 INFO  epoch 75 training [time: 0.58s, train loss: 0.2600]
Fri 21 Feb 2025 12:35:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:40 INFO  epoch 76 training [time: 0.35s, train loss: 0.2229]
Fri 21 Feb 2025 12:35:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:41 INFO  epoch 77 training [time: 0.41s, train loss: 0.2317]
Fri 21 Feb 2025 12:35:41 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:41 INFO  epoch 78 training [time: 0.71s, train loss: 0.2086]
Fri 21 Feb 2025 12:35:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:42 INFO  epoch 79 training [time: 0.34s, train loss: 0.2399]
Fri 21 Feb 2025 12:35:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:42 INFO  epoch 80 training [time: 0.50s, train loss: 0.2097]
Fri 21 Feb 2025 12:35:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:43 INFO  epoch 81 training [time: 0.50s, train loss: 0.2262]
Fri 21 Feb 2025 12:35:43 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:43 INFO  epoch 82 training [time: 0.40s, train loss: 0.2250]
Fri 21 Feb 2025 12:35:43 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:44 INFO  epoch 83 training [time: 0.53s, train loss: 0.1775]
Fri 21 Feb 2025 12:35:44 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:44 INFO  epoch 84 training [time: 0.48s, train loss: 0.1812]
Fri 21 Feb 2025 12:35:44 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:45 INFO  epoch 85 training [time: 0.39s, train loss: 0.1961]
Fri 21 Feb 2025 12:35:45 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:45 INFO  epoch 86 training [time: 0.63s, train loss: 0.2141]
Fri 21 Feb 2025 12:35:46 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:46 INFO  epoch 87 training [time: 0.41s, train loss: 0.2021]
Fri 21 Feb 2025 12:35:46 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:47 INFO  epoch 88 training [time: 0.56s, train loss: 0.1947]
Fri 21 Feb 2025 12:35:47 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:47 INFO  epoch 89 training [time: 0.52s, train loss: 0.2414]
Fri 21 Feb 2025 12:35:47 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:48 INFO  epoch 90 training [time: 0.44s, train loss: 0.1655]
Fri 21 Feb 2025 12:35:48 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:48 INFO  epoch 91 training [time: 0.46s, train loss: 0.1935]
Fri 21 Feb 2025 12:35:48 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:48 INFO  epoch 92 training [time: 0.29s, train loss: 0.2168]
Fri 21 Feb 2025 12:35:48 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:49 INFO  epoch 93 training [time: 0.26s, train loss: 0.1819]
Fri 21 Feb 2025 12:35:49 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:49 INFO  epoch 94 training [time: 0.38s, train loss: 0.1823]
Fri 21 Feb 2025 12:35:49 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:49 INFO  epoch 95 training [time: 0.30s, train loss: 0.2095]
Fri 21 Feb 2025 12:35:49 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:50 INFO  epoch 96 training [time: 0.47s, train loss: 0.2009]
Fri 21 Feb 2025 12:35:50 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:50 INFO  epoch 97 training [time: 0.38s, train loss: 0.1697]
Fri 21 Feb 2025 12:35:50 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:51 INFO  epoch 98 training [time: 0.33s, train loss: 0.1698]
Fri 21 Feb 2025 12:35:51 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:51 INFO  epoch 99 training [time: 0.31s, train loss: 0.1522]
Fri 21 Feb 2025 12:35:51 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:51 INFO  Loading model structure and parameters from saved/NeuMF-Feb-21-2025_12-35-05.pth
Fri 21 Feb 2025 12:35:53 INFO  The running environment of this training is as follows:
+-------------+----------------+
| Environment |     Usage      |
+=============+================+
| CPU         |    27.10 %     |
+-------------+----------------+
| GPU         |   0.0 / 0.0    |
+-------------+----------------+
| Memory      | 0.58 G/24.00 G |
+-------------+----------------+
Fri 21 Feb 2025 12:35:53 INFO  best valid : None
Fri 21 Feb 2025 12:35:53 INFO  test result: OrderedDict([('hit@5', 0.8061), ('hit@10', 0.8251), ('hit@20', 0.8367), ('mrr@5', 0.5206), ('mrr@10', 0.5233), ('mrr@20', 0.5241), ('map@5', 0.5206), ('map@10', 0.5233), ('map@20', 0.5241), ('precision@5', 0.1612), ('precision@10', 0.0825), ('precision@20', 0.0418), ('recall@5', 0.8061), ('recall@10', 0.8251), ('recall@20', 0.8367)])
Fri 21 Feb 2025 12:41:54 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 12:41:54 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = MRR@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
mf_embedding_size = 64
mlp_embedding_size = 64
mlp_hidden_size = [128, 64]
dropout_prob = 0.1
mf_train = True
mlp_train = True
use_pretrain = False
mf_pretrain_path = None
mlp_pretrain_path = None
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.GENERAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


Fri 21 Feb 2025 12:41:55 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 12:41:55 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 12:41:55 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
Fri 21 Feb 2025 12:41:55 INFO  NeuMF(
  (user_mf_embedding): Embedding(982, 64)
  (item_mf_embedding): Embedding(85, 64)
  (user_mlp_embedding): Embedding(982, 64)
  (item_mlp_embedding): Embedding(85, 64)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=128, out_features=64, bias=True)
      (5): ReLU()
    )
  )
  (predict_layer): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 161473
Fri 21 Feb 2025 12:41:55 INFO  FLOPs: 24960.0
Fri 21 Feb 2025 12:41:55 INFO  epoch 0 training [time: 0.37s, train loss: 2.7709]
Fri 21 Feb 2025 12:41:55 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:56 INFO  epoch 1 training [time: 0.47s, train loss: 2.7649]
Fri 21 Feb 2025 12:41:56 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:56 INFO  epoch 2 training [time: 0.36s, train loss: 2.7500]
Fri 21 Feb 2025 12:41:56 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:57 INFO  epoch 3 training [time: 0.59s, train loss: 2.7209]
Fri 21 Feb 2025 12:41:57 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:57 INFO  epoch 4 training [time: 0.33s, train loss: 2.6628]
Fri 21 Feb 2025 12:41:57 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:57 INFO  epoch 5 training [time: 0.42s, train loss: 2.5754]
Fri 21 Feb 2025 12:41:57 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:58 INFO  epoch 6 training [time: 0.41s, train loss: 2.4645]
Fri 21 Feb 2025 12:41:58 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:58 INFO  epoch 7 training [time: 0.42s, train loss: 2.3115]
Fri 21 Feb 2025 12:41:58 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:59 INFO  epoch 8 training [time: 0.30s, train loss: 2.1386]
Fri 21 Feb 2025 12:41:59 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:59 INFO  epoch 9 training [time: 0.38s, train loss: 1.8873]
Fri 21 Feb 2025 12:41:59 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:41:59 INFO  epoch 10 training [time: 0.38s, train loss: 1.5939]
Fri 21 Feb 2025 12:41:59 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:00 INFO  epoch 11 training [time: 0.55s, train loss: 1.2955]
Fri 21 Feb 2025 12:42:00 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:00 INFO  epoch 12 training [time: 0.37s, train loss: 1.0705]
Fri 21 Feb 2025 12:42:00 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:01 INFO  epoch 13 training [time: 0.39s, train loss: 0.9338]
Fri 21 Feb 2025 12:42:01 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:01 INFO  epoch 14 training [time: 0.30s, train loss: 0.8442]
Fri 21 Feb 2025 12:42:01 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:02 INFO  epoch 15 training [time: 0.52s, train loss: 0.7504]
Fri 21 Feb 2025 12:42:02 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:02 INFO  epoch 16 training [time: 0.52s, train loss: 0.7390]
Fri 21 Feb 2025 12:42:02 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:03 INFO  epoch 17 training [time: 0.34s, train loss: 0.6031]
Fri 21 Feb 2025 12:42:03 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:03 INFO  epoch 18 training [time: 0.40s, train loss: 0.7931]
Fri 21 Feb 2025 12:42:03 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:03 INFO  epoch 19 training [time: 0.27s, train loss: 0.5367]
Fri 21 Feb 2025 12:42:03 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:04 INFO  epoch 20 training [time: 0.38s, train loss: 0.5918]
Fri 21 Feb 2025 12:42:04 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:04 INFO  epoch 21 training [time: 0.37s, train loss: 0.4985]
Fri 21 Feb 2025 12:42:04 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:05 INFO  epoch 22 training [time: 0.37s, train loss: 0.5018]
Fri 21 Feb 2025 12:42:05 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:05 INFO  epoch 23 training [time: 0.30s, train loss: 0.5896]
Fri 21 Feb 2025 12:42:05 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:05 INFO  epoch 24 training [time: 0.36s, train loss: 0.5333]
Fri 21 Feb 2025 12:42:05 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:06 INFO  epoch 25 training [time: 0.41s, train loss: 0.4834]
Fri 21 Feb 2025 12:42:06 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:06 INFO  epoch 26 training [time: 0.32s, train loss: 0.4815]
Fri 21 Feb 2025 12:42:06 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:06 INFO  epoch 27 training [time: 0.43s, train loss: 0.5282]
Fri 21 Feb 2025 12:42:06 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:07 INFO  epoch 28 training [time: 0.52s, train loss: 0.4716]
Fri 21 Feb 2025 12:42:07 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:08 INFO  epoch 29 training [time: 0.51s, train loss: 0.5044]
Fri 21 Feb 2025 12:42:08 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:08 INFO  epoch 30 training [time: 0.50s, train loss: 0.4346]
Fri 21 Feb 2025 12:42:08 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:09 INFO  epoch 31 training [time: 0.53s, train loss: 0.4846]
Fri 21 Feb 2025 12:42:09 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:09 INFO  epoch 32 training [time: 0.51s, train loss: 0.3593]
Fri 21 Feb 2025 12:42:09 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:09 INFO  epoch 33 training [time: 0.32s, train loss: 0.4492]
Fri 21 Feb 2025 12:42:10 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:10 INFO  epoch 34 training [time: 0.60s, train loss: 0.4701]
Fri 21 Feb 2025 12:42:10 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:11 INFO  epoch 35 training [time: 0.44s, train loss: 0.4505]
Fri 21 Feb 2025 12:42:11 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:11 INFO  epoch 36 training [time: 0.64s, train loss: 0.5018]
Fri 21 Feb 2025 12:42:11 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:12 INFO  epoch 37 training [time: 0.52s, train loss: 0.5086]
Fri 21 Feb 2025 12:42:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:12 INFO  epoch 38 training [time: 0.32s, train loss: 0.3964]
Fri 21 Feb 2025 12:42:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:13 INFO  epoch 39 training [time: 0.70s, train loss: 0.4194]
Fri 21 Feb 2025 12:42:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:13 INFO  epoch 40 training [time: 0.29s, train loss: 0.4199]
Fri 21 Feb 2025 12:42:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:14 INFO  epoch 41 training [time: 0.38s, train loss: 0.3737]
Fri 21 Feb 2025 12:42:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:14 INFO  epoch 42 training [time: 0.50s, train loss: 0.3773]
Fri 21 Feb 2025 12:42:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:15 INFO  epoch 43 training [time: 0.56s, train loss: 0.4120]
Fri 21 Feb 2025 12:42:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:15 INFO  epoch 44 training [time: 0.47s, train loss: 0.3731]
Fri 21 Feb 2025 12:42:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:16 INFO  epoch 45 training [time: 0.51s, train loss: 0.3781]
Fri 21 Feb 2025 12:42:16 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:16 INFO  epoch 46 training [time: 0.40s, train loss: 0.3543]
Fri 21 Feb 2025 12:42:16 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:17 INFO  epoch 47 training [time: 0.35s, train loss: 0.3406]
Fri 21 Feb 2025 12:42:17 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:17 INFO  epoch 48 training [time: 0.51s, train loss: 0.3259]
Fri 21 Feb 2025 12:42:17 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:18 INFO  epoch 49 training [time: 0.49s, train loss: 0.4157]
Fri 21 Feb 2025 12:42:18 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:18 INFO  epoch 50 training [time: 0.43s, train loss: 0.3342]
Fri 21 Feb 2025 12:42:18 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:19 INFO  epoch 51 training [time: 0.53s, train loss: 0.3787]
Fri 21 Feb 2025 12:42:19 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:19 INFO  epoch 52 training [time: 0.32s, train loss: 0.3343]
Fri 21 Feb 2025 12:42:19 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:19 INFO  epoch 53 training [time: 0.38s, train loss: 0.3083]
Fri 21 Feb 2025 12:42:19 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:20 INFO  epoch 54 training [time: 0.46s, train loss: 0.3006]
Fri 21 Feb 2025 12:42:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:20 INFO  epoch 55 training [time: 0.41s, train loss: 0.3132]
Fri 21 Feb 2025 12:42:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:21 INFO  epoch 56 training [time: 0.32s, train loss: 0.3101]
Fri 21 Feb 2025 12:42:21 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:21 INFO  epoch 57 training [time: 0.57s, train loss: 0.3171]
Fri 21 Feb 2025 12:42:21 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:22 INFO  epoch 58 training [time: 0.40s, train loss: 0.3510]
Fri 21 Feb 2025 12:42:22 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:22 INFO  epoch 59 training [time: 0.58s, train loss: 0.3106]
Fri 21 Feb 2025 12:42:22 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:23 INFO  epoch 60 training [time: 0.50s, train loss: 0.2966]
Fri 21 Feb 2025 12:42:23 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:23 INFO  epoch 61 training [time: 0.37s, train loss: 0.3329]
Fri 21 Feb 2025 12:42:23 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:24 INFO  epoch 62 training [time: 0.37s, train loss: 0.3636]
Fri 21 Feb 2025 12:42:24 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:24 INFO  epoch 63 training [time: 0.35s, train loss: 0.2938]
Fri 21 Feb 2025 12:42:24 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:24 INFO  epoch 64 training [time: 0.47s, train loss: 0.2787]
Fri 21 Feb 2025 12:42:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:25 INFO  epoch 65 training [time: 0.53s, train loss: 0.3149]
Fri 21 Feb 2025 12:42:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:26 INFO  epoch 66 training [time: 0.62s, train loss: 0.2468]
Fri 21 Feb 2025 12:42:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:26 INFO  epoch 67 training [time: 0.53s, train loss: 0.2614]
Fri 21 Feb 2025 12:42:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:27 INFO  epoch 68 training [time: 0.38s, train loss: 0.2680]
Fri 21 Feb 2025 12:42:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:27 INFO  epoch 69 training [time: 0.41s, train loss: 0.2756]
Fri 21 Feb 2025 12:42:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:28 INFO  epoch 70 training [time: 0.48s, train loss: 0.2147]
Fri 21 Feb 2025 12:42:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:28 INFO  epoch 71 training [time: 0.38s, train loss: 0.2777]
Fri 21 Feb 2025 12:42:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:28 INFO  epoch 72 training [time: 0.38s, train loss: 0.2498]
Fri 21 Feb 2025 12:42:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:29 INFO  epoch 73 training [time: 0.51s, train loss: 0.2464]
Fri 21 Feb 2025 12:42:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:29 INFO  epoch 74 training [time: 0.49s, train loss: 0.2457]
Fri 21 Feb 2025 12:42:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:30 INFO  epoch 75 training [time: 0.66s, train loss: 0.2600]
Fri 21 Feb 2025 12:42:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:31 INFO  epoch 76 training [time: 0.44s, train loss: 0.2229]
Fri 21 Feb 2025 12:42:31 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:31 INFO  epoch 77 training [time: 0.39s, train loss: 0.2317]
Fri 21 Feb 2025 12:42:31 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:32 INFO  epoch 78 training [time: 0.69s, train loss: 0.2086]
Fri 21 Feb 2025 12:42:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:32 INFO  epoch 79 training [time: 0.57s, train loss: 0.2399]
Fri 21 Feb 2025 12:42:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:33 INFO  epoch 80 training [time: 0.32s, train loss: 0.2097]
Fri 21 Feb 2025 12:42:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:33 INFO  epoch 81 training [time: 0.36s, train loss: 0.2262]
Fri 21 Feb 2025 12:42:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:33 INFO  epoch 82 training [time: 0.35s, train loss: 0.2250]
Fri 21 Feb 2025 12:42:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:34 INFO  epoch 83 training [time: 0.32s, train loss: 0.1775]
Fri 21 Feb 2025 12:42:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:34 INFO  epoch 84 training [time: 0.48s, train loss: 0.1812]
Fri 21 Feb 2025 12:42:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:35 INFO  epoch 85 training [time: 0.46s, train loss: 0.1961]
Fri 21 Feb 2025 12:42:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:35 INFO  epoch 86 training [time: 0.68s, train loss: 0.2141]
Fri 21 Feb 2025 12:42:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:36 INFO  epoch 87 training [time: 0.52s, train loss: 0.2021]
Fri 21 Feb 2025 12:42:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:36 INFO  epoch 88 training [time: 0.40s, train loss: 0.1947]
Fri 21 Feb 2025 12:42:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:37 INFO  epoch 89 training [time: 0.37s, train loss: 0.2414]
Fri 21 Feb 2025 12:42:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:37 INFO  epoch 90 training [time: 0.61s, train loss: 0.1655]
Fri 21 Feb 2025 12:42:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:38 INFO  epoch 91 training [time: 0.37s, train loss: 0.1935]
Fri 21 Feb 2025 12:42:38 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:38 INFO  epoch 92 training [time: 0.44s, train loss: 0.2168]
Fri 21 Feb 2025 12:42:38 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:39 INFO  epoch 93 training [time: 0.43s, train loss: 0.1819]
Fri 21 Feb 2025 12:42:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:39 INFO  epoch 94 training [time: 0.40s, train loss: 0.1823]
Fri 21 Feb 2025 12:42:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:40 INFO  epoch 95 training [time: 0.52s, train loss: 0.2095]
Fri 21 Feb 2025 12:42:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:40 INFO  epoch 96 training [time: 0.55s, train loss: 0.2009]
Fri 21 Feb 2025 12:42:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:41 INFO  epoch 97 training [time: 0.33s, train loss: 0.1697]
Fri 21 Feb 2025 12:42:41 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:41 INFO  epoch 98 training [time: 0.39s, train loss: 0.1698]
Fri 21 Feb 2025 12:42:41 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:42 INFO  epoch 99 training [time: 0.71s, train loss: 0.1522]
Fri 21 Feb 2025 12:42:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:42 INFO  Loading model structure and parameters from saved/NeuMF-Feb-21-2025_12-41-55.pth
Fri 21 Feb 2025 12:42:44 INFO  The running environment of this training is as follows:
+-------------+----------------+
| Environment |     Usage      |
+=============+================+
| CPU         |    24.40 %     |
+-------------+----------------+
| GPU         |   0.0 / 0.0    |
+-------------+----------------+
| Memory      | 0.59 G/24.00 G |
+-------------+----------------+
Fri 21 Feb 2025 12:42:44 INFO  best valid : None
Fri 21 Feb 2025 12:42:44 INFO  test result: OrderedDict([('hit@5', 0.8061), ('hit@10', 0.8251), ('hit@20', 0.8367), ('mrr@5', 0.5206), ('mrr@10', 0.5233), ('mrr@20', 0.5241), ('map@5', 0.5206), ('map@10', 0.5233), ('map@20', 0.5241), ('precision@5', 0.1612), ('precision@10', 0.0825), ('precision@20', 0.0418), ('recall@5', 0.8061), ('recall@10', 0.8251), ('recall@20', 0.8367)])
Fri 21 Feb 2025 12:47:26 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 12:47:26 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 2020
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = MRR@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
mf_embedding_size = 64
mlp_embedding_size = 64
mlp_hidden_size = [128, 64]
dropout_prob = 0.1
mf_train = True
mlp_train = True
use_pretrain = False
mf_pretrain_path = None
mlp_pretrain_path = None
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.GENERAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


Fri 21 Feb 2025 12:47:26 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 12:47:26 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 12:47:26 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
Fri 21 Feb 2025 12:47:26 INFO  NeuMF(
  (user_mf_embedding): Embedding(982, 64)
  (item_mf_embedding): Embedding(85, 64)
  (user_mlp_embedding): Embedding(982, 64)
  (item_mlp_embedding): Embedding(85, 64)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=128, out_features=64, bias=True)
      (5): ReLU()
    )
  )
  (predict_layer): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 161473
Fri 21 Feb 2025 12:47:26 INFO  FLOPs: 24960.0
Fri 21 Feb 2025 12:47:27 INFO  epoch 0 training [time: 0.41s, train loss: 2.7714]
Fri 21 Feb 2025 12:47:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:27 INFO  epoch 1 training [time: 0.46s, train loss: 2.7665]
Fri 21 Feb 2025 12:47:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:28 INFO  epoch 2 training [time: 0.37s, train loss: 2.7550]
Fri 21 Feb 2025 12:47:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:28 INFO  epoch 3 training [time: 0.33s, train loss: 2.7259]
Fri 21 Feb 2025 12:47:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:28 INFO  epoch 4 training [time: 0.36s, train loss: 2.6791]
Fri 21 Feb 2025 12:47:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:29 INFO  epoch 5 training [time: 0.62s, train loss: 2.6079]
Fri 21 Feb 2025 12:47:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:29 INFO  epoch 6 training [time: 0.34s, train loss: 2.5014]
Fri 21 Feb 2025 12:47:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:30 INFO  epoch 7 training [time: 0.60s, train loss: 2.3835]
Fri 21 Feb 2025 12:47:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:31 INFO  epoch 8 training [time: 0.62s, train loss: 2.2069]
Fri 21 Feb 2025 12:47:31 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:31 INFO  epoch 9 training [time: 0.35s, train loss: 2.0033]
Fri 21 Feb 2025 12:47:31 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:32 INFO  epoch 10 training [time: 0.61s, train loss: 1.7077]
Fri 21 Feb 2025 12:47:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:32 INFO  epoch 11 training [time: 0.39s, train loss: 1.4823]
Fri 21 Feb 2025 12:47:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:33 INFO  epoch 12 training [time: 0.69s, train loss: 1.1231]
Fri 21 Feb 2025 12:47:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:33 INFO  epoch 13 training [time: 0.43s, train loss: 0.9101]
Fri 21 Feb 2025 12:47:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:34 INFO  epoch 14 training [time: 0.59s, train loss: 0.7599]
Fri 21 Feb 2025 12:47:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:34 INFO  epoch 15 training [time: 0.47s, train loss: 0.7086]
Fri 21 Feb 2025 12:47:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:35 INFO  epoch 16 training [time: 0.51s, train loss: 0.6301]
Fri 21 Feb 2025 12:47:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:35 INFO  epoch 17 training [time: 0.43s, train loss: 0.6109]
Fri 21 Feb 2025 12:47:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:36 INFO  epoch 18 training [time: 0.49s, train loss: 0.5601]
Fri 21 Feb 2025 12:47:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:37 INFO  epoch 19 training [time: 0.77s, train loss: 0.6461]
Fri 21 Feb 2025 12:47:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:37 INFO  epoch 20 training [time: 0.38s, train loss: 0.6115]
Fri 21 Feb 2025 12:47:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:37 INFO  epoch 21 training [time: 0.38s, train loss: 0.5151]
Fri 21 Feb 2025 12:47:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:38 INFO  epoch 22 training [time: 0.37s, train loss: 0.5103]
Fri 21 Feb 2025 12:47:38 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:38 INFO  epoch 23 training [time: 0.36s, train loss: 0.5217]
Fri 21 Feb 2025 12:47:38 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:39 INFO  epoch 24 training [time: 0.41s, train loss: 0.5715]
Fri 21 Feb 2025 12:47:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:39 INFO  epoch 25 training [time: 0.38s, train loss: 0.5707]
Fri 21 Feb 2025 12:47:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:40 INFO  epoch 26 training [time: 0.54s, train loss: 0.4733]
Fri 21 Feb 2025 12:47:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:40 INFO  epoch 27 training [time: 0.52s, train loss: 0.4733]
Fri 21 Feb 2025 12:47:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:41 INFO  epoch 28 training [time: 0.47s, train loss: 0.5117]
Fri 21 Feb 2025 12:47:41 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:41 INFO  epoch 29 training [time: 0.50s, train loss: 0.4992]
Fri 21 Feb 2025 12:47:41 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:42 INFO  epoch 30 training [time: 0.35s, train loss: 0.4419]
Fri 21 Feb 2025 12:47:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:42 INFO  epoch 31 training [time: 0.58s, train loss: 0.4351]
Fri 21 Feb 2025 12:47:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:43 INFO  epoch 32 training [time: 0.65s, train loss: 0.5159]
Fri 21 Feb 2025 12:47:43 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:43 INFO  epoch 33 training [time: 0.41s, train loss: 0.4743]
Fri 21 Feb 2025 12:47:43 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:44 INFO  epoch 34 training [time: 0.51s, train loss: 0.4512]
Fri 21 Feb 2025 12:47:44 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:44 INFO  epoch 35 training [time: 0.36s, train loss: 0.4699]
Fri 21 Feb 2025 12:47:44 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:45 INFO  epoch 36 training [time: 0.48s, train loss: 0.4484]
Fri 21 Feb 2025 12:47:45 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:45 INFO  epoch 37 training [time: 0.46s, train loss: 0.3731]
Fri 21 Feb 2025 12:47:45 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:46 INFO  epoch 38 training [time: 0.46s, train loss: 0.3858]
Fri 21 Feb 2025 12:47:46 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:46 INFO  epoch 39 training [time: 0.46s, train loss: 0.4375]
Fri 21 Feb 2025 12:47:46 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:47 INFO  epoch 40 training [time: 0.52s, train loss: 0.4255]
Fri 21 Feb 2025 12:47:47 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:47 INFO  epoch 41 training [time: 0.34s, train loss: 0.3588]
Fri 21 Feb 2025 12:47:47 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:48 INFO  epoch 42 training [time: 0.47s, train loss: 0.3695]
Fri 21 Feb 2025 12:47:48 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:48 INFO  epoch 43 training [time: 0.61s, train loss: 0.4123]
Fri 21 Feb 2025 12:47:48 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:49 INFO  epoch 44 training [time: 0.37s, train loss: 0.3613]
Fri 21 Feb 2025 12:47:49 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:49 INFO  epoch 45 training [time: 0.30s, train loss: 0.3716]
Fri 21 Feb 2025 12:47:49 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:49 INFO  epoch 46 training [time: 0.41s, train loss: 0.3771]
Fri 21 Feb 2025 12:47:49 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:50 INFO  epoch 47 training [time: 0.52s, train loss: 0.3284]
Fri 21 Feb 2025 12:47:50 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:50 INFO  epoch 48 training [time: 0.52s, train loss: 0.3863]
Fri 21 Feb 2025 12:47:50 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:51 INFO  epoch 49 training [time: 0.41s, train loss: 0.2951]
Fri 21 Feb 2025 12:47:51 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:51 INFO  epoch 50 training [time: 0.50s, train loss: 0.3428]
Fri 21 Feb 2025 12:47:51 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:52 INFO  epoch 51 training [time: 0.34s, train loss: 0.3375]
Fri 21 Feb 2025 12:47:52 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:52 INFO  epoch 52 training [time: 0.44s, train loss: 0.3802]
Fri 21 Feb 2025 12:47:52 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:53 INFO  epoch 53 training [time: 0.60s, train loss: 0.4414]
Fri 21 Feb 2025 12:47:53 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:53 INFO  epoch 54 training [time: 0.37s, train loss: 0.2979]
Fri 21 Feb 2025 12:47:53 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:54 INFO  epoch 55 training [time: 0.36s, train loss: 0.3058]
Fri 21 Feb 2025 12:47:54 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:54 INFO  epoch 56 training [time: 0.40s, train loss: 0.2747]
Fri 21 Feb 2025 12:47:54 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:55 INFO  epoch 57 training [time: 0.48s, train loss: 0.2740]
Fri 21 Feb 2025 12:47:55 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:55 INFO  epoch 58 training [time: 0.34s, train loss: 0.2740]
Fri 21 Feb 2025 12:47:55 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:55 INFO  epoch 59 training [time: 0.46s, train loss: 0.3246]
Fri 21 Feb 2025 12:47:56 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:56 INFO  epoch 60 training [time: 0.22s, train loss: 0.2273]
Fri 21 Feb 2025 12:47:56 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:56 INFO  epoch 61 training [time: 0.42s, train loss: 0.2644]
Fri 21 Feb 2025 12:47:56 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:57 INFO  epoch 62 training [time: 0.39s, train loss: 0.2821]
Fri 21 Feb 2025 12:47:57 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:57 INFO  epoch 63 training [time: 0.40s, train loss: 0.2621]
Fri 21 Feb 2025 12:47:57 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:57 INFO  epoch 64 training [time: 0.31s, train loss: 0.3063]
Fri 21 Feb 2025 12:47:57 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:58 INFO  epoch 65 training [time: 0.35s, train loss: 0.2529]
Fri 21 Feb 2025 12:47:58 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:58 INFO  epoch 66 training [time: 0.43s, train loss: 0.3235]
Fri 21 Feb 2025 12:47:58 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:59 INFO  epoch 67 training [time: 0.46s, train loss: 0.2782]
Fri 21 Feb 2025 12:47:59 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:47:59 INFO  epoch 68 training [time: 0.50s, train loss: 0.2574]
Fri 21 Feb 2025 12:47:59 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:00 INFO  epoch 69 training [time: 0.56s, train loss: 0.3221]
Fri 21 Feb 2025 12:48:00 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:00 INFO  epoch 70 training [time: 0.54s, train loss: 0.2674]
Fri 21 Feb 2025 12:48:00 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:01 INFO  epoch 71 training [time: 0.37s, train loss: 0.2330]
Fri 21 Feb 2025 12:48:01 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:02 INFO  epoch 72 training [time: 0.71s, train loss: 0.2973]
Fri 21 Feb 2025 12:48:02 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:02 INFO  epoch 73 training [time: 0.55s, train loss: 0.2330]
Fri 21 Feb 2025 12:48:02 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:03 INFO  epoch 74 training [time: 0.56s, train loss: 0.3018]
Fri 21 Feb 2025 12:48:03 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:03 INFO  epoch 75 training [time: 0.50s, train loss: 0.2292]
Fri 21 Feb 2025 12:48:03 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:04 INFO  epoch 76 training [time: 0.58s, train loss: 0.2247]
Fri 21 Feb 2025 12:48:04 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:04 INFO  epoch 77 training [time: 0.61s, train loss: 0.2228]
Fri 21 Feb 2025 12:48:04 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:05 INFO  epoch 78 training [time: 0.45s, train loss: 0.2280]
Fri 21 Feb 2025 12:48:05 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:05 INFO  epoch 79 training [time: 0.47s, train loss: 0.2230]
Fri 21 Feb 2025 12:48:05 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:06 INFO  epoch 80 training [time: 0.36s, train loss: 0.2196]
Fri 21 Feb 2025 12:48:06 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:07 INFO  epoch 81 training [time: 0.68s, train loss: 0.2646]
Fri 21 Feb 2025 12:48:07 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:07 INFO  epoch 82 training [time: 0.44s, train loss: 0.2293]
Fri 21 Feb 2025 12:48:07 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:08 INFO  epoch 83 training [time: 0.56s, train loss: 0.2035]
Fri 21 Feb 2025 12:48:08 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:08 INFO  epoch 84 training [time: 0.46s, train loss: 0.2048]
Fri 21 Feb 2025 12:48:08 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:09 INFO  epoch 85 training [time: 0.69s, train loss: 0.1948]
Fri 21 Feb 2025 12:48:09 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:09 INFO  epoch 86 training [time: 0.49s, train loss: 0.2547]
Fri 21 Feb 2025 12:48:09 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:10 INFO  epoch 87 training [time: 0.28s, train loss: 0.2289]
Fri 21 Feb 2025 12:48:10 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:10 INFO  epoch 88 training [time: 0.56s, train loss: 0.1940]
Fri 21 Feb 2025 12:48:10 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:11 INFO  epoch 89 training [time: 0.47s, train loss: 0.1972]
Fri 21 Feb 2025 12:48:11 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:11 INFO  epoch 90 training [time: 0.51s, train loss: 0.2085]
Fri 21 Feb 2025 12:48:11 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:12 INFO  epoch 91 training [time: 0.59s, train loss: 0.2234]
Fri 21 Feb 2025 12:48:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:12 INFO  epoch 92 training [time: 0.55s, train loss: 0.1669]
Fri 21 Feb 2025 12:48:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:13 INFO  epoch 93 training [time: 0.40s, train loss: 0.1815]
Fri 21 Feb 2025 12:48:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:13 INFO  epoch 94 training [time: 0.47s, train loss: 0.2144]
Fri 21 Feb 2025 12:48:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:14 INFO  epoch 95 training [time: 0.35s, train loss: 0.1817]
Fri 21 Feb 2025 12:48:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:14 INFO  epoch 96 training [time: 0.32s, train loss: 0.1826]
Fri 21 Feb 2025 12:48:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:15 INFO  epoch 97 training [time: 0.50s, train loss: 0.1822]
Fri 21 Feb 2025 12:48:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:15 INFO  epoch 98 training [time: 0.43s, train loss: 0.1919]
Fri 21 Feb 2025 12:48:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:15 INFO  epoch 99 training [time: 0.37s, train loss: 0.1843]
Fri 21 Feb 2025 12:48:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:15 INFO  Loading model structure and parameters from saved/NeuMF-Feb-21-2025_12-47-26.pth
Fri 21 Feb 2025 12:48:17 INFO  The running environment of this training is as follows:
+-------------+----------------+
| Environment |     Usage      |
+=============+================+
| CPU         |    24.50 %     |
+-------------+----------------+
| GPU         |   0.0 / 0.0    |
+-------------+----------------+
| Memory      | 0.60 G/24.00 G |
+-------------+----------------+
Fri 21 Feb 2025 12:48:17 INFO  best valid : None
Fri 21 Feb 2025 12:48:17 INFO  test result: OrderedDict([('hit@5', 0.8082), ('hit@10', 0.8188), ('hit@20', 0.8567), ('mrr@5', 0.5287), ('mrr@10', 0.5302), ('mrr@20', 0.5328), ('map@5', 0.5287), ('map@10', 0.5302), ('map@20', 0.5328), ('precision@5', 0.1616), ('precision@10', 0.0819), ('precision@20', 0.0428), ('recall@5', 0.8082), ('recall@10', 0.8188), ('recall@20', 0.8567)])
Fri 21 Feb 2025 13:09:11 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 13:09:11 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = MRR@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
mf_embedding_size = 64
mlp_embedding_size = 64
mlp_hidden_size = [128, 64]
dropout_prob = 0.1
mf_train = True
mlp_train = True
use_pretrain = False
mf_pretrain_path = None
mlp_pretrain_path = None
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.GENERAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


Fri 21 Feb 2025 13:09:11 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 13:09:11 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 13:09:11 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
Fri 21 Feb 2025 13:09:11 INFO  NeuMF(
  (user_mf_embedding): Embedding(982, 64)
  (item_mf_embedding): Embedding(85, 64)
  (user_mlp_embedding): Embedding(982, 64)
  (item_mlp_embedding): Embedding(85, 64)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=128, out_features=64, bias=True)
      (5): ReLU()
    )
  )
  (predict_layer): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 161473
Fri 21 Feb 2025 13:09:11 INFO  FLOPs: 24960.0
Fri 21 Feb 2025 13:09:12 INFO  epoch 0 training [time: 0.41s, train loss: 2.7709]
Fri 21 Feb 2025 13:09:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:12 INFO  epoch 1 training [time: 0.42s, train loss: 2.7649]
Fri 21 Feb 2025 13:09:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:13 INFO  epoch 2 training [time: 0.58s, train loss: 2.7500]
Fri 21 Feb 2025 13:09:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:13 INFO  epoch 3 training [time: 0.28s, train loss: 2.7209]
Fri 21 Feb 2025 13:09:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:14 INFO  epoch 4 training [time: 0.51s, train loss: 2.6628]
Fri 21 Feb 2025 13:09:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:14 INFO  epoch 5 training [time: 0.40s, train loss: 2.5754]
Fri 21 Feb 2025 13:09:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:15 INFO  epoch 6 training [time: 0.47s, train loss: 2.4645]
Fri 21 Feb 2025 13:09:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:15 INFO  epoch 7 training [time: 0.30s, train loss: 2.3115]
Fri 21 Feb 2025 13:09:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:15 INFO  epoch 8 training [time: 0.47s, train loss: 2.1386]
Fri 21 Feb 2025 13:09:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:16 INFO  epoch 9 training [time: 0.37s, train loss: 1.8873]
Fri 21 Feb 2025 13:09:16 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:16 INFO  epoch 10 training [time: 0.39s, train loss: 1.5939]
Fri 21 Feb 2025 13:09:16 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:17 INFO  epoch 11 training [time: 0.34s, train loss: 1.2955]
Fri 21 Feb 2025 13:09:17 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:17 INFO  epoch 12 training [time: 0.57s, train loss: 1.0705]
Fri 21 Feb 2025 13:09:17 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:18 INFO  epoch 13 training [time: 0.33s, train loss: 0.9338]
Fri 21 Feb 2025 13:09:18 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:18 INFO  epoch 14 training [time: 0.44s, train loss: 0.8442]
Fri 21 Feb 2025 13:09:18 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:18 INFO  epoch 15 training [time: 0.32s, train loss: 0.7504]
Fri 21 Feb 2025 13:09:18 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:19 INFO  epoch 16 training [time: 0.32s, train loss: 0.7390]
Fri 21 Feb 2025 13:09:19 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:19 INFO  epoch 17 training [time: 0.50s, train loss: 0.6031]
Fri 21 Feb 2025 13:09:19 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:20 INFO  epoch 18 training [time: 0.37s, train loss: 0.7931]
Fri 21 Feb 2025 13:09:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:20 INFO  epoch 19 training [time: 0.39s, train loss: 0.5367]
Fri 21 Feb 2025 13:09:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:20 INFO  epoch 20 training [time: 0.28s, train loss: 0.5918]
Fri 21 Feb 2025 13:09:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:21 INFO  epoch 21 training [time: 0.61s, train loss: 0.4985]
Fri 21 Feb 2025 13:09:21 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:21 INFO  epoch 22 training [time: 0.37s, train loss: 0.5018]
Fri 21 Feb 2025 13:09:21 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:22 INFO  epoch 23 training [time: 0.38s, train loss: 0.5896]
Fri 21 Feb 2025 13:09:22 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:22 INFO  epoch 24 training [time: 0.36s, train loss: 0.5333]
Fri 21 Feb 2025 13:09:22 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:23 INFO  epoch 25 training [time: 0.43s, train loss: 0.4834]
Fri 21 Feb 2025 13:09:23 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:23 INFO  epoch 26 training [time: 0.59s, train loss: 0.4815]
Fri 21 Feb 2025 13:09:23 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:24 INFO  epoch 27 training [time: 0.31s, train loss: 0.5282]
Fri 21 Feb 2025 13:09:24 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:24 INFO  epoch 28 training [time: 0.66s, train loss: 0.4716]
Fri 21 Feb 2025 13:09:24 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:25 INFO  epoch 29 training [time: 0.40s, train loss: 0.5044]
Fri 21 Feb 2025 13:09:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:25 INFO  epoch 30 training [time: 0.46s, train loss: 0.4346]
Fri 21 Feb 2025 13:09:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:26 INFO  epoch 31 training [time: 0.48s, train loss: 0.4846]
Fri 21 Feb 2025 13:09:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:26 INFO  epoch 32 training [time: 0.36s, train loss: 0.3593]
Fri 21 Feb 2025 13:09:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:27 INFO  epoch 33 training [time: 0.54s, train loss: 0.4492]
Fri 21 Feb 2025 13:09:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:27 INFO  epoch 34 training [time: 0.44s, train loss: 0.4701]
Fri 21 Feb 2025 13:09:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:28 INFO  epoch 35 training [time: 0.44s, train loss: 0.4505]
Fri 21 Feb 2025 13:09:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:28 INFO  epoch 36 training [time: 0.48s, train loss: 0.5018]
Fri 21 Feb 2025 13:09:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:28 INFO  epoch 37 training [time: 0.41s, train loss: 0.5086]
Fri 21 Feb 2025 13:09:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:29 INFO  epoch 38 training [time: 0.40s, train loss: 0.3964]
Fri 21 Feb 2025 13:09:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:29 INFO  epoch 39 training [time: 0.26s, train loss: 0.4194]
Fri 21 Feb 2025 13:09:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:30 INFO  epoch 40 training [time: 0.34s, train loss: 0.4199]
Fri 21 Feb 2025 13:09:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:30 INFO  epoch 41 training [time: 0.38s, train loss: 0.3737]
Fri 21 Feb 2025 13:09:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:30 INFO  epoch 42 training [time: 0.32s, train loss: 0.3773]
Fri 21 Feb 2025 13:09:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:31 INFO  epoch 43 training [time: 0.62s, train loss: 0.4120]
Fri 21 Feb 2025 13:09:31 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:32 INFO  epoch 44 training [time: 0.56s, train loss: 0.3731]
Fri 21 Feb 2025 13:09:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:32 INFO  epoch 45 training [time: 0.37s, train loss: 0.3781]
Fri 21 Feb 2025 13:09:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:32 INFO  epoch 46 training [time: 0.43s, train loss: 0.3543]
Fri 21 Feb 2025 13:09:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:33 INFO  epoch 47 training [time: 0.48s, train loss: 0.3406]
Fri 21 Feb 2025 13:09:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:34 INFO  epoch 48 training [time: 0.59s, train loss: 0.3259]
Fri 21 Feb 2025 13:09:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:34 INFO  epoch 49 training [time: 0.49s, train loss: 0.4157]
Fri 21 Feb 2025 13:09:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:35 INFO  epoch 50 training [time: 0.58s, train loss: 0.3342]
Fri 21 Feb 2025 13:09:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:35 INFO  epoch 51 training [time: 0.40s, train loss: 0.3787]
Fri 21 Feb 2025 13:09:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:35 INFO  epoch 52 training [time: 0.34s, train loss: 0.3343]
Fri 21 Feb 2025 13:09:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:36 INFO  epoch 53 training [time: 0.45s, train loss: 0.3083]
Fri 21 Feb 2025 13:09:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:36 INFO  epoch 54 training [time: 0.49s, train loss: 0.3006]
Fri 21 Feb 2025 13:09:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:37 INFO  epoch 55 training [time: 0.58s, train loss: 0.3132]
Fri 21 Feb 2025 13:09:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:37 INFO  epoch 56 training [time: 0.33s, train loss: 0.3101]
Fri 21 Feb 2025 13:09:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:38 INFO  epoch 57 training [time: 0.45s, train loss: 0.3171]
Fri 21 Feb 2025 13:09:38 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:38 INFO  epoch 58 training [time: 0.52s, train loss: 0.3510]
Fri 21 Feb 2025 13:09:38 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:39 INFO  epoch 59 training [time: 0.52s, train loss: 0.3106]
Fri 21 Feb 2025 13:09:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:39 INFO  epoch 60 training [time: 0.38s, train loss: 0.2966]
Fri 21 Feb 2025 13:09:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:40 INFO  epoch 61 training [time: 0.38s, train loss: 0.3329]
Fri 21 Feb 2025 13:09:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:40 INFO  epoch 62 training [time: 0.29s, train loss: 0.3636]
Fri 21 Feb 2025 13:09:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:41 INFO  epoch 63 training [time: 0.38s, train loss: 0.2938]
Fri 21 Feb 2025 13:09:41 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:41 INFO  epoch 64 training [time: 0.61s, train loss: 0.2787]
Fri 21 Feb 2025 13:09:41 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:42 INFO  epoch 65 training [time: 0.55s, train loss: 0.3149]
Fri 21 Feb 2025 13:09:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:42 INFO  epoch 66 training [time: 0.48s, train loss: 0.2468]
Fri 21 Feb 2025 13:09:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:43 INFO  epoch 67 training [time: 0.49s, train loss: 0.2614]
Fri 21 Feb 2025 13:09:43 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:43 INFO  epoch 68 training [time: 0.34s, train loss: 0.2680]
Fri 21 Feb 2025 13:09:43 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:44 INFO  epoch 69 training [time: 0.38s, train loss: 0.2756]
Fri 21 Feb 2025 13:09:44 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:44 INFO  epoch 70 training [time: 0.31s, train loss: 0.2147]
Fri 21 Feb 2025 13:09:44 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:44 INFO  epoch 71 training [time: 0.51s, train loss: 0.2777]
Fri 21 Feb 2025 13:09:44 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:45 INFO  epoch 72 training [time: 0.36s, train loss: 0.2498]
Fri 21 Feb 2025 13:09:45 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:45 INFO  epoch 73 training [time: 0.55s, train loss: 0.2464]
Fri 21 Feb 2025 13:09:45 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:46 INFO  epoch 74 training [time: 0.38s, train loss: 0.2457]
Fri 21 Feb 2025 13:09:46 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:47 INFO  epoch 75 training [time: 0.83s, train loss: 0.2600]
Fri 21 Feb 2025 13:09:47 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:47 INFO  epoch 76 training [time: 0.42s, train loss: 0.2229]
Fri 21 Feb 2025 13:09:47 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:48 INFO  epoch 77 training [time: 0.58s, train loss: 0.2317]
Fri 21 Feb 2025 13:09:48 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:48 INFO  epoch 78 training [time: 0.28s, train loss: 0.2086]
Fri 21 Feb 2025 13:09:48 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:48 INFO  epoch 79 training [time: 0.38s, train loss: 0.2399]
Fri 21 Feb 2025 13:09:48 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:49 INFO  epoch 80 training [time: 0.49s, train loss: 0.2097]
Fri 21 Feb 2025 13:09:49 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:49 INFO  epoch 81 training [time: 0.30s, train loss: 0.2262]
Fri 21 Feb 2025 13:09:49 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:50 INFO  epoch 82 training [time: 0.29s, train loss: 0.2250]
Fri 21 Feb 2025 13:09:50 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:50 INFO  epoch 83 training [time: 0.50s, train loss: 0.1775]
Fri 21 Feb 2025 13:09:50 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:51 INFO  epoch 84 training [time: 0.62s, train loss: 0.1812]
Fri 21 Feb 2025 13:09:51 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:51 INFO  epoch 85 training [time: 0.53s, train loss: 0.1961]
Fri 21 Feb 2025 13:09:51 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:52 INFO  epoch 86 training [time: 0.38s, train loss: 0.2141]
Fri 21 Feb 2025 13:09:52 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:52 INFO  epoch 87 training [time: 0.32s, train loss: 0.2021]
Fri 21 Feb 2025 13:09:52 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:52 INFO  epoch 88 training [time: 0.34s, train loss: 0.1947]
Fri 21 Feb 2025 13:09:52 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:53 INFO  epoch 89 training [time: 0.39s, train loss: 0.2414]
Fri 21 Feb 2025 13:09:53 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:53 INFO  epoch 90 training [time: 0.50s, train loss: 0.1655]
Fri 21 Feb 2025 13:09:53 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:54 INFO  epoch 91 training [time: 0.60s, train loss: 0.1935]
Fri 21 Feb 2025 13:09:54 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:54 INFO  epoch 92 training [time: 0.44s, train loss: 0.2168]
Fri 21 Feb 2025 13:09:54 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:55 INFO  epoch 93 training [time: 0.40s, train loss: 0.1819]
Fri 21 Feb 2025 13:09:55 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:55 INFO  epoch 94 training [time: 0.52s, train loss: 0.1823]
Fri 21 Feb 2025 13:09:55 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:56 INFO  epoch 95 training [time: 0.53s, train loss: 0.2095]
Fri 21 Feb 2025 13:09:56 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:56 INFO  epoch 96 training [time: 0.35s, train loss: 0.2009]
Fri 21 Feb 2025 13:09:56 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:57 INFO  epoch 97 training [time: 0.44s, train loss: 0.1697]
Fri 21 Feb 2025 13:09:57 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:57 INFO  epoch 98 training [time: 0.45s, train loss: 0.1698]
Fri 21 Feb 2025 13:09:57 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:58 INFO  epoch 99 training [time: 0.40s, train loss: 0.1522]
Fri 21 Feb 2025 13:09:58 INFO  Saving current: saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:58 INFO  Loading model structure and parameters from saved/NeuMF-Feb-21-2025_13-09-11.pth
Fri 21 Feb 2025 13:09:59 INFO  The running environment of this training is as follows:
+-------------+----------------+
| Environment |     Usage      |
+=============+================+
| CPU         |    29.00 %     |
+-------------+----------------+
| GPU         |   0.0 / 0.0    |
+-------------+----------------+
| Memory      | 0.62 G/24.00 G |
+-------------+----------------+
Fri 21 Feb 2025 13:09:59 INFO  best valid : None
Fri 21 Feb 2025 13:09:59 INFO  test result: OrderedDict([('hit@5', 0.8061), ('hit@10', 0.8251), ('hit@20', 0.8367), ('mrr@5', 0.5206), ('mrr@10', 0.5233), ('mrr@20', 0.5241), ('map@5', 0.5206), ('map@10', 0.5233), ('map@20', 0.5241), ('precision@5', 0.1612), ('precision@10', 0.0825), ('precision@20', 0.0418), ('recall@5', 0.8061), ('recall@10', 0.8251), ('recall@20', 0.8367)])
Fri 21 Feb 2025 19:33:57 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Fri 21 Feb 2025 19:33:57 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 0
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = MRR@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 3

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
mf_embedding_size = 64
mlp_embedding_size = 64
mlp_hidden_size = [128, 64]
dropout_prob = 0.1
mf_train = True
mlp_train = True
use_pretrain = False
mf_pretrain_path = None
mlp_pretrain_path = None
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.GENERAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


Fri 21 Feb 2025 19:33:57 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Fri 21 Feb 2025 19:33:57 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]
Fri 21 Feb 2025 19:33:57 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
Fri 21 Feb 2025 19:33:57 INFO  NeuMF(
  (user_mf_embedding): Embedding(982, 64)
  (item_mf_embedding): Embedding(85, 64)
  (user_mlp_embedding): Embedding(982, 64)
  (item_mlp_embedding): Embedding(85, 64)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=128, out_features=64, bias=True)
      (5): ReLU()
    )
  )
  (predict_layer): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 161473
Fri 21 Feb 2025 19:33:57 INFO  FLOPs: 24960.0
Fri 21 Feb 2025 19:33:58 INFO  epoch 0 training [time: 0.45s, train loss: 2.7709]
Fri 21 Feb 2025 19:33:58 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:33:58 INFO  epoch 1 training [time: 0.42s, train loss: 2.7649]
Fri 21 Feb 2025 19:33:58 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:33:59 INFO  epoch 2 training [time: 0.46s, train loss: 2.7500]
Fri 21 Feb 2025 19:33:59 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:33:59 INFO  epoch 3 training [time: 0.53s, train loss: 2.7209]
Fri 21 Feb 2025 19:33:59 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:00 INFO  epoch 4 training [time: 0.35s, train loss: 2.6628]
Fri 21 Feb 2025 19:34:00 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:00 INFO  epoch 5 training [time: 0.43s, train loss: 2.5754]
Fri 21 Feb 2025 19:34:00 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:01 INFO  epoch 6 training [time: 0.37s, train loss: 2.4645]
Fri 21 Feb 2025 19:34:01 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:01 INFO  epoch 7 training [time: 0.65s, train loss: 2.3115]
Fri 21 Feb 2025 19:34:01 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:02 INFO  epoch 8 training [time: 0.53s, train loss: 2.1386]
Fri 21 Feb 2025 19:34:02 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:02 INFO  epoch 9 training [time: 0.45s, train loss: 1.8873]
Fri 21 Feb 2025 19:34:02 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:03 INFO  epoch 10 training [time: 0.47s, train loss: 1.5939]
Fri 21 Feb 2025 19:34:03 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:03 INFO  epoch 11 training [time: 0.30s, train loss: 1.2955]
Fri 21 Feb 2025 19:34:03 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:04 INFO  epoch 12 training [time: 0.32s, train loss: 1.0705]
Fri 21 Feb 2025 19:34:04 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:04 INFO  epoch 13 training [time: 0.48s, train loss: 0.9338]
Fri 21 Feb 2025 19:34:04 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:05 INFO  epoch 14 training [time: 0.56s, train loss: 0.8442]
Fri 21 Feb 2025 19:34:05 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:05 INFO  epoch 15 training [time: 0.38s, train loss: 0.7504]
Fri 21 Feb 2025 19:34:05 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:05 INFO  epoch 16 training [time: 0.29s, train loss: 0.7390]
Fri 21 Feb 2025 19:34:05 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:06 INFO  epoch 17 training [time: 0.50s, train loss: 0.6031]
Fri 21 Feb 2025 19:34:06 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:06 INFO  epoch 18 training [time: 0.40s, train loss: 0.7931]
Fri 21 Feb 2025 19:34:06 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:07 INFO  epoch 19 training [time: 0.36s, train loss: 0.5367]
Fri 21 Feb 2025 19:34:07 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:07 INFO  epoch 20 training [time: 0.36s, train loss: 0.5918]
Fri 21 Feb 2025 19:34:07 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:07 INFO  epoch 21 training [time: 0.36s, train loss: 0.4985]
Fri 21 Feb 2025 19:34:07 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:08 INFO  epoch 22 training [time: 0.51s, train loss: 0.5018]
Fri 21 Feb 2025 19:34:08 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:08 INFO  epoch 23 training [time: 0.39s, train loss: 0.5896]
Fri 21 Feb 2025 19:34:08 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:09 INFO  epoch 24 training [time: 0.36s, train loss: 0.5333]
Fri 21 Feb 2025 19:34:09 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:09 INFO  epoch 25 training [time: 0.52s, train loss: 0.4834]
Fri 21 Feb 2025 19:34:09 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:10 INFO  epoch 26 training [time: 0.40s, train loss: 0.4815]
Fri 21 Feb 2025 19:34:10 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:10 INFO  epoch 27 training [time: 0.44s, train loss: 0.5282]
Fri 21 Feb 2025 19:34:10 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:11 INFO  epoch 28 training [time: 0.54s, train loss: 0.4716]
Fri 21 Feb 2025 19:34:11 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:11 INFO  epoch 29 training [time: 0.53s, train loss: 0.5044]
Fri 21 Feb 2025 19:34:11 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:12 INFO  epoch 30 training [time: 0.63s, train loss: 0.4346]
Fri 21 Feb 2025 19:34:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:12 INFO  epoch 31 training [time: 0.36s, train loss: 0.4846]
Fri 21 Feb 2025 19:34:12 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:13 INFO  epoch 32 training [time: 0.35s, train loss: 0.3593]
Fri 21 Feb 2025 19:34:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:13 INFO  epoch 33 training [time: 0.42s, train loss: 0.4492]
Fri 21 Feb 2025 19:34:13 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:14 INFO  epoch 34 training [time: 0.35s, train loss: 0.4701]
Fri 21 Feb 2025 19:34:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:14 INFO  epoch 35 training [time: 0.46s, train loss: 0.4505]
Fri 21 Feb 2025 19:34:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:14 INFO  epoch 36 training [time: 0.38s, train loss: 0.5018]
Fri 21 Feb 2025 19:34:14 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:15 INFO  epoch 37 training [time: 0.41s, train loss: 0.5086]
Fri 21 Feb 2025 19:34:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:15 INFO  epoch 38 training [time: 0.49s, train loss: 0.3964]
Fri 21 Feb 2025 19:34:15 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:16 INFO  epoch 39 training [time: 0.34s, train loss: 0.4194]
Fri 21 Feb 2025 19:34:16 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:16 INFO  epoch 40 training [time: 0.47s, train loss: 0.4199]
Fri 21 Feb 2025 19:34:16 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:17 INFO  epoch 41 training [time: 0.41s, train loss: 0.3737]
Fri 21 Feb 2025 19:34:17 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:17 INFO  epoch 42 training [time: 0.65s, train loss: 0.3773]
Fri 21 Feb 2025 19:34:17 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:18 INFO  epoch 43 training [time: 0.39s, train loss: 0.4120]
Fri 21 Feb 2025 19:34:18 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:18 INFO  epoch 44 training [time: 0.42s, train loss: 0.3731]
Fri 21 Feb 2025 19:34:18 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:19 INFO  epoch 45 training [time: 0.61s, train loss: 0.3781]
Fri 21 Feb 2025 19:34:19 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:19 INFO  epoch 46 training [time: 0.36s, train loss: 0.3543]
Fri 21 Feb 2025 19:34:19 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:20 INFO  epoch 47 training [time: 0.60s, train loss: 0.3406]
Fri 21 Feb 2025 19:34:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:20 INFO  epoch 48 training [time: 0.43s, train loss: 0.3259]
Fri 21 Feb 2025 19:34:20 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:21 INFO  epoch 49 training [time: 0.33s, train loss: 0.4157]
Fri 21 Feb 2025 19:34:21 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:21 INFO  epoch 50 training [time: 0.48s, train loss: 0.3342]
Fri 21 Feb 2025 19:34:21 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:22 INFO  epoch 51 training [time: 0.53s, train loss: 0.3787]
Fri 21 Feb 2025 19:34:22 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:22 INFO  epoch 52 training [time: 0.68s, train loss: 0.3343]
Fri 21 Feb 2025 19:34:22 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:23 INFO  epoch 53 training [time: 0.36s, train loss: 0.3083]
Fri 21 Feb 2025 19:34:23 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:23 INFO  epoch 54 training [time: 0.59s, train loss: 0.3006]
Fri 21 Feb 2025 19:34:23 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:24 INFO  epoch 55 training [time: 0.46s, train loss: 0.3132]
Fri 21 Feb 2025 19:34:24 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:24 INFO  epoch 56 training [time: 0.48s, train loss: 0.3101]
Fri 21 Feb 2025 19:34:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:25 INFO  epoch 57 training [time: 0.52s, train loss: 0.3171]
Fri 21 Feb 2025 19:34:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:25 INFO  epoch 58 training [time: 0.34s, train loss: 0.3510]
Fri 21 Feb 2025 19:34:25 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:26 INFO  epoch 59 training [time: 0.49s, train loss: 0.3106]
Fri 21 Feb 2025 19:34:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:26 INFO  epoch 60 training [time: 0.37s, train loss: 0.2966]
Fri 21 Feb 2025 19:34:26 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:27 INFO  epoch 61 training [time: 0.25s, train loss: 0.3329]
Fri 21 Feb 2025 19:34:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:27 INFO  epoch 62 training [time: 0.53s, train loss: 0.3636]
Fri 21 Feb 2025 19:34:27 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:28 INFO  epoch 63 training [time: 0.51s, train loss: 0.2938]
Fri 21 Feb 2025 19:34:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:28 INFO  epoch 64 training [time: 0.48s, train loss: 0.2787]
Fri 21 Feb 2025 19:34:28 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:29 INFO  epoch 65 training [time: 0.40s, train loss: 0.3149]
Fri 21 Feb 2025 19:34:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:29 INFO  epoch 66 training [time: 0.52s, train loss: 0.2468]
Fri 21 Feb 2025 19:34:29 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:30 INFO  epoch 67 training [time: 0.54s, train loss: 0.2614]
Fri 21 Feb 2025 19:34:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:30 INFO  epoch 68 training [time: 0.48s, train loss: 0.2680]
Fri 21 Feb 2025 19:34:30 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:31 INFO  epoch 69 training [time: 0.50s, train loss: 0.2756]
Fri 21 Feb 2025 19:34:31 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:31 INFO  epoch 70 training [time: 0.38s, train loss: 0.2147]
Fri 21 Feb 2025 19:34:31 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:32 INFO  epoch 71 training [time: 0.62s, train loss: 0.2777]
Fri 21 Feb 2025 19:34:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:32 INFO  epoch 72 training [time: 0.44s, train loss: 0.2498]
Fri 21 Feb 2025 19:34:32 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:33 INFO  epoch 73 training [time: 0.40s, train loss: 0.2464]
Fri 21 Feb 2025 19:34:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:33 INFO  epoch 74 training [time: 0.24s, train loss: 0.2457]
Fri 21 Feb 2025 19:34:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:33 INFO  epoch 75 training [time: 0.45s, train loss: 0.2600]
Fri 21 Feb 2025 19:34:33 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:34 INFO  epoch 76 training [time: 0.34s, train loss: 0.2229]
Fri 21 Feb 2025 19:34:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:34 INFO  epoch 77 training [time: 0.37s, train loss: 0.2317]
Fri 21 Feb 2025 19:34:34 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:35 INFO  epoch 78 training [time: 0.53s, train loss: 0.2086]
Fri 21 Feb 2025 19:34:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:35 INFO  epoch 79 training [time: 0.48s, train loss: 0.2399]
Fri 21 Feb 2025 19:34:35 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:36 INFO  epoch 80 training [time: 0.60s, train loss: 0.2097]
Fri 21 Feb 2025 19:34:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:36 INFO  epoch 81 training [time: 0.46s, train loss: 0.2262]
Fri 21 Feb 2025 19:34:36 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:37 INFO  epoch 82 training [time: 0.35s, train loss: 0.2250]
Fri 21 Feb 2025 19:34:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:37 INFO  epoch 83 training [time: 0.45s, train loss: 0.1775]
Fri 21 Feb 2025 19:34:37 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:38 INFO  epoch 84 training [time: 0.70s, train loss: 0.1812]
Fri 21 Feb 2025 19:34:38 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:39 INFO  epoch 85 training [time: 0.64s, train loss: 0.1961]
Fri 21 Feb 2025 19:34:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:39 INFO  epoch 86 training [time: 0.45s, train loss: 0.2141]
Fri 21 Feb 2025 19:34:39 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:40 INFO  epoch 87 training [time: 0.51s, train loss: 0.2021]
Fri 21 Feb 2025 19:34:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:40 INFO  epoch 88 training [time: 0.27s, train loss: 0.1947]
Fri 21 Feb 2025 19:34:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:40 INFO  epoch 89 training [time: 0.39s, train loss: 0.2414]
Fri 21 Feb 2025 19:34:40 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:41 INFO  epoch 90 training [time: 0.60s, train loss: 0.1655]
Fri 21 Feb 2025 19:34:41 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:41 INFO  epoch 91 training [time: 0.37s, train loss: 0.1935]
Fri 21 Feb 2025 19:34:41 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:42 INFO  epoch 92 training [time: 0.48s, train loss: 0.2168]
Fri 21 Feb 2025 19:34:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:42 INFO  epoch 93 training [time: 0.40s, train loss: 0.1819]
Fri 21 Feb 2025 19:34:42 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:43 INFO  epoch 94 training [time: 0.46s, train loss: 0.1823]
Fri 21 Feb 2025 19:34:43 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:43 INFO  epoch 95 training [time: 0.43s, train loss: 0.2095]
Fri 21 Feb 2025 19:34:43 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:44 INFO  epoch 96 training [time: 0.55s, train loss: 0.2009]
Fri 21 Feb 2025 19:34:44 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:44 INFO  epoch 97 training [time: 0.71s, train loss: 0.1697]
Fri 21 Feb 2025 19:34:44 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:45 INFO  epoch 98 training [time: 0.67s, train loss: 0.1698]
Fri 21 Feb 2025 19:34:45 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:46 INFO  epoch 99 training [time: 0.53s, train loss: 0.1522]
Fri 21 Feb 2025 19:34:46 INFO  Saving current: saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:46 INFO  Loading model structure and parameters from saved/NeuMF-Feb-21-2025_19-33-57.pth
Fri 21 Feb 2025 19:34:48 INFO  The running environment of this training is as follows:
+-------------+----------------+
| Environment |     Usage      |
+=============+================+
| CPU         |    26.10 %     |
+-------------+----------------+
| GPU         |   0.0 / 0.0    |
+-------------+----------------+
| Memory      | 0.56 G/24.00 G |
+-------------+----------------+
Fri 21 Feb 2025 19:34:48 INFO  best valid : None
Fri 21 Feb 2025 19:34:48 INFO  test result: OrderedDict([('hit@5', 0.806), ('hit@10', 0.825), ('hit@20', 0.837), ('mrr@5', 0.521), ('mrr@10', 0.523), ('mrr@20', 0.524), ('map@5', 0.521), ('map@10', 0.523), ('map@20', 0.524), ('precision@5', 0.161), ('precision@10', 0.083), ('precision@20', 0.042), ('recall@5', 0.806), ('recall@10', 0.825), ('recall@20', 0.837)])
Tue 25 Feb 2025 15:34:05 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Tue 25 Feb 2025 15:34:05 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 2020
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = MRR@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 3

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
mf_embedding_size = 64
mlp_embedding_size = 64
mlp_hidden_size = [128, 64]
dropout_prob = 0.1
mf_train = True
mlp_train = True
use_pretrain = False
mf_pretrain_path = None
mlp_pretrain_path = None
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.GENERAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


Tue 25 Feb 2025 15:34:05 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Tue 25 Feb 2025 15:34:05 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]
Tue 25 Feb 2025 15:34:05 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
Tue 25 Feb 2025 15:34:05 INFO  NeuMF(
  (user_mf_embedding): Embedding(982, 64)
  (item_mf_embedding): Embedding(85, 64)
  (user_mlp_embedding): Embedding(982, 64)
  (item_mlp_embedding): Embedding(85, 64)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=128, out_features=64, bias=True)
      (5): ReLU()
    )
  )
  (predict_layer): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 161473
Tue 25 Feb 2025 15:34:05 INFO  FLOPs: 24960.0
Tue 25 Feb 2025 15:34:06 INFO  epoch 0 training [time: 0.59s, train loss: 2.7714]
Tue 25 Feb 2025 15:34:06 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:07 INFO  epoch 1 training [time: 0.68s, train loss: 2.7665]
Tue 25 Feb 2025 15:34:07 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:07 INFO  epoch 2 training [time: 0.65s, train loss: 2.7550]
Tue 25 Feb 2025 15:34:07 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:08 INFO  epoch 3 training [time: 0.46s, train loss: 2.7259]
Tue 25 Feb 2025 15:34:08 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:08 INFO  epoch 4 training [time: 0.53s, train loss: 2.6791]
Tue 25 Feb 2025 15:34:08 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:09 INFO  epoch 5 training [time: 0.41s, train loss: 2.6079]
Tue 25 Feb 2025 15:34:09 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:09 INFO  epoch 6 training [time: 0.42s, train loss: 2.5014]
Tue 25 Feb 2025 15:34:09 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:10 INFO  epoch 7 training [time: 0.38s, train loss: 2.3835]
Tue 25 Feb 2025 15:34:10 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:10 INFO  epoch 8 training [time: 0.44s, train loss: 2.2069]
Tue 25 Feb 2025 15:34:10 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:11 INFO  epoch 9 training [time: 0.54s, train loss: 2.0033]
Tue 25 Feb 2025 15:34:11 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:11 INFO  epoch 10 training [time: 0.49s, train loss: 1.7077]
Tue 25 Feb 2025 15:34:11 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:12 INFO  epoch 11 training [time: 0.53s, train loss: 1.4823]
Tue 25 Feb 2025 15:34:12 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:12 INFO  epoch 12 training [time: 0.66s, train loss: 1.1231]
Tue 25 Feb 2025 15:34:12 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:13 INFO  epoch 13 training [time: 0.52s, train loss: 0.9101]
Tue 25 Feb 2025 15:34:13 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:14 INFO  epoch 14 training [time: 0.86s, train loss: 0.7599]
Tue 25 Feb 2025 15:34:14 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:14 INFO  epoch 15 training [time: 0.54s, train loss: 0.7086]
Tue 25 Feb 2025 15:34:14 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:15 INFO  epoch 16 training [time: 0.64s, train loss: 0.6301]
Tue 25 Feb 2025 15:34:15 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:16 INFO  epoch 17 training [time: 0.56s, train loss: 0.6109]
Tue 25 Feb 2025 15:34:16 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:16 INFO  epoch 18 training [time: 0.50s, train loss: 0.5601]
Tue 25 Feb 2025 15:34:16 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:17 INFO  epoch 19 training [time: 0.48s, train loss: 0.6461]
Tue 25 Feb 2025 15:34:17 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:17 INFO  epoch 20 training [time: 0.61s, train loss: 0.6115]
Tue 25 Feb 2025 15:34:17 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:18 INFO  epoch 21 training [time: 0.48s, train loss: 0.5151]
Tue 25 Feb 2025 15:34:18 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:18 INFO  epoch 22 training [time: 0.45s, train loss: 0.5103]
Tue 25 Feb 2025 15:34:18 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:19 INFO  epoch 23 training [time: 0.43s, train loss: 0.5217]
Tue 25 Feb 2025 15:34:19 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:19 INFO  epoch 24 training [time: 0.51s, train loss: 0.5715]
Tue 25 Feb 2025 15:34:19 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:20 INFO  epoch 25 training [time: 0.39s, train loss: 0.5707]
Tue 25 Feb 2025 15:34:20 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:20 INFO  epoch 26 training [time: 0.35s, train loss: 0.4733]
Tue 25 Feb 2025 15:34:20 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:21 INFO  epoch 27 training [time: 0.55s, train loss: 0.4733]
Tue 25 Feb 2025 15:34:21 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:21 INFO  epoch 28 training [time: 0.26s, train loss: 0.5117]
Tue 25 Feb 2025 15:34:21 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:21 INFO  epoch 29 training [time: 0.37s, train loss: 0.4992]
Tue 25 Feb 2025 15:34:21 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:22 INFO  epoch 30 training [time: 0.31s, train loss: 0.4419]
Tue 25 Feb 2025 15:34:22 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:22 INFO  epoch 31 training [time: 0.39s, train loss: 0.4351]
Tue 25 Feb 2025 15:34:22 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:23 INFO  epoch 32 training [time: 0.42s, train loss: 0.5159]
Tue 25 Feb 2025 15:34:23 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:23 INFO  epoch 33 training [time: 0.53s, train loss: 0.4743]
Tue 25 Feb 2025 15:34:23 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:24 INFO  epoch 34 training [time: 0.43s, train loss: 0.4512]
Tue 25 Feb 2025 15:34:24 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:24 INFO  epoch 35 training [time: 0.55s, train loss: 0.4699]
Tue 25 Feb 2025 15:34:24 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:25 INFO  epoch 36 training [time: 0.48s, train loss: 0.4484]
Tue 25 Feb 2025 15:34:25 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:25 INFO  epoch 37 training [time: 0.49s, train loss: 0.3731]
Tue 25 Feb 2025 15:34:25 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:26 INFO  epoch 38 training [time: 0.45s, train loss: 0.3858]
Tue 25 Feb 2025 15:34:26 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:26 INFO  epoch 39 training [time: 0.47s, train loss: 0.4375]
Tue 25 Feb 2025 15:34:26 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:27 INFO  epoch 40 training [time: 0.74s, train loss: 0.4255]
Tue 25 Feb 2025 15:34:27 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:27 INFO  epoch 41 training [time: 0.54s, train loss: 0.3588]
Tue 25 Feb 2025 15:34:27 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:28 INFO  epoch 42 training [time: 0.53s, train loss: 0.3695]
Tue 25 Feb 2025 15:34:28 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:29 INFO  epoch 43 training [time: 0.53s, train loss: 0.4123]
Tue 25 Feb 2025 15:34:29 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:29 INFO  epoch 44 training [time: 0.45s, train loss: 0.3613]
Tue 25 Feb 2025 15:34:29 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:30 INFO  epoch 45 training [time: 0.46s, train loss: 0.3716]
Tue 25 Feb 2025 15:34:30 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:30 INFO  epoch 46 training [time: 0.40s, train loss: 0.3771]
Tue 25 Feb 2025 15:34:30 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:31 INFO  epoch 47 training [time: 0.64s, train loss: 0.3284]
Tue 25 Feb 2025 15:34:31 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:31 INFO  epoch 48 training [time: 0.53s, train loss: 0.3863]
Tue 25 Feb 2025 15:34:31 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:32 INFO  epoch 49 training [time: 0.51s, train loss: 0.2951]
Tue 25 Feb 2025 15:34:32 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:32 INFO  epoch 50 training [time: 0.52s, train loss: 0.3428]
Tue 25 Feb 2025 15:34:32 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:33 INFO  epoch 51 training [time: 0.59s, train loss: 0.3375]
Tue 25 Feb 2025 15:34:33 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:34 INFO  epoch 52 training [time: 0.62s, train loss: 0.3802]
Tue 25 Feb 2025 15:34:34 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:34 INFO  epoch 53 training [time: 0.43s, train loss: 0.4414]
Tue 25 Feb 2025 15:34:34 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:35 INFO  epoch 54 training [time: 0.55s, train loss: 0.2979]
Tue 25 Feb 2025 15:34:35 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:35 INFO  epoch 55 training [time: 0.39s, train loss: 0.3058]
Tue 25 Feb 2025 15:34:35 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:36 INFO  epoch 56 training [time: 0.61s, train loss: 0.2747]
Tue 25 Feb 2025 15:34:36 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:36 INFO  epoch 57 training [time: 0.37s, train loss: 0.2740]
Tue 25 Feb 2025 15:34:36 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:36 INFO  epoch 58 training [time: 0.48s, train loss: 0.2740]
Tue 25 Feb 2025 15:34:37 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:37 INFO  epoch 59 training [time: 0.31s, train loss: 0.3246]
Tue 25 Feb 2025 15:34:37 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:37 INFO  epoch 60 training [time: 0.36s, train loss: 0.2273]
Tue 25 Feb 2025 15:34:37 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:38 INFO  epoch 61 training [time: 0.65s, train loss: 0.2644]
Tue 25 Feb 2025 15:34:38 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:38 INFO  epoch 62 training [time: 0.32s, train loss: 0.2821]
Tue 25 Feb 2025 15:34:38 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:39 INFO  epoch 63 training [time: 0.43s, train loss: 0.2621]
Tue 25 Feb 2025 15:34:39 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:39 INFO  epoch 64 training [time: 0.51s, train loss: 0.3063]
Tue 25 Feb 2025 15:34:39 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:40 INFO  epoch 65 training [time: 0.38s, train loss: 0.2529]
Tue 25 Feb 2025 15:34:40 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:40 INFO  epoch 66 training [time: 0.48s, train loss: 0.3235]
Tue 25 Feb 2025 15:34:40 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:41 INFO  epoch 67 training [time: 0.51s, train loss: 0.2782]
Tue 25 Feb 2025 15:34:41 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:41 INFO  epoch 68 training [time: 0.59s, train loss: 0.2574]
Tue 25 Feb 2025 15:34:41 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:42 INFO  epoch 69 training [time: 0.47s, train loss: 0.3221]
Tue 25 Feb 2025 15:34:42 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:42 INFO  epoch 70 training [time: 0.48s, train loss: 0.2674]
Tue 25 Feb 2025 15:34:42 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:43 INFO  epoch 71 training [time: 0.50s, train loss: 0.2330]
Tue 25 Feb 2025 15:34:43 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:43 INFO  epoch 72 training [time: 0.42s, train loss: 0.2973]
Tue 25 Feb 2025 15:34:43 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:44 INFO  epoch 73 training [time: 0.50s, train loss: 0.2330]
Tue 25 Feb 2025 15:34:44 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:44 INFO  epoch 74 training [time: 0.45s, train loss: 0.3018]
Tue 25 Feb 2025 15:34:44 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:45 INFO  epoch 75 training [time: 0.57s, train loss: 0.2292]
Tue 25 Feb 2025 15:34:45 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:46 INFO  epoch 76 training [time: 1.05s, train loss: 0.2247]
Tue 25 Feb 2025 15:34:46 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:47 INFO  epoch 77 training [time: 0.58s, train loss: 0.2228]
Tue 25 Feb 2025 15:34:47 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:47 INFO  epoch 78 training [time: 0.59s, train loss: 0.2280]
Tue 25 Feb 2025 15:34:47 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:48 INFO  epoch 79 training [time: 0.54s, train loss: 0.2230]
Tue 25 Feb 2025 15:34:48 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:49 INFO  epoch 80 training [time: 0.71s, train loss: 0.2196]
Tue 25 Feb 2025 15:34:49 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:49 INFO  epoch 81 training [time: 0.75s, train loss: 0.2646]
Tue 25 Feb 2025 15:34:49 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:50 INFO  epoch 82 training [time: 0.47s, train loss: 0.2293]
Tue 25 Feb 2025 15:34:50 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:50 INFO  epoch 83 training [time: 0.50s, train loss: 0.2035]
Tue 25 Feb 2025 15:34:50 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:51 INFO  epoch 84 training [time: 0.41s, train loss: 0.2048]
Tue 25 Feb 2025 15:34:51 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:51 INFO  epoch 85 training [time: 0.64s, train loss: 0.1948]
Tue 25 Feb 2025 15:34:51 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:52 INFO  epoch 86 training [time: 0.61s, train loss: 0.2547]
Tue 25 Feb 2025 15:34:52 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:53 INFO  epoch 87 training [time: 0.55s, train loss: 0.2289]
Tue 25 Feb 2025 15:34:53 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:53 INFO  epoch 88 training [time: 0.43s, train loss: 0.1940]
Tue 25 Feb 2025 15:34:53 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:54 INFO  epoch 89 training [time: 0.61s, train loss: 0.1972]
Tue 25 Feb 2025 15:34:54 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:54 INFO  epoch 90 training [time: 0.64s, train loss: 0.2085]
Tue 25 Feb 2025 15:34:54 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:55 INFO  epoch 91 training [time: 0.34s, train loss: 0.2234]
Tue 25 Feb 2025 15:34:55 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:55 INFO  epoch 92 training [time: 0.63s, train loss: 0.1669]
Tue 25 Feb 2025 15:34:55 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:56 INFO  epoch 93 training [time: 0.36s, train loss: 0.1815]
Tue 25 Feb 2025 15:34:56 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:56 INFO  epoch 94 training [time: 0.68s, train loss: 0.2144]
Tue 25 Feb 2025 15:34:56 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:57 INFO  epoch 95 training [time: 0.46s, train loss: 0.1817]
Tue 25 Feb 2025 15:34:57 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:58 INFO  epoch 96 training [time: 0.68s, train loss: 0.1826]
Tue 25 Feb 2025 15:34:58 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:58 INFO  epoch 97 training [time: 0.34s, train loss: 0.1822]
Tue 25 Feb 2025 15:34:58 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:58 INFO  epoch 98 training [time: 0.40s, train loss: 0.1919]
Tue 25 Feb 2025 15:34:58 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:59 INFO  epoch 99 training [time: 0.67s, train loss: 0.1843]
Tue 25 Feb 2025 15:34:59 INFO  Saving current: saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:34:59 INFO  Loading model structure and parameters from saved/NeuMF-Feb-25-2025_15-34-05.pth
Tue 25 Feb 2025 15:35:01 INFO  The running environment of this training is as follows:
+-------------+----------------+
| Environment |     Usage      |
+=============+================+
| CPU         |    23.50 %     |
+-------------+----------------+
| GPU         |   0.0 / 0.0    |
+-------------+----------------+
| Memory      | 0.33 G/24.00 G |
+-------------+----------------+
Tue 25 Feb 2025 15:35:01 INFO  best valid : None
Tue 25 Feb 2025 15:35:01 INFO  test result: OrderedDict([('hit@5', 0.808), ('hit@10', 0.819), ('hit@20', 0.857), ('mrr@5', 0.529), ('mrr@10', 0.53), ('mrr@20', 0.533), ('map@5', 0.529), ('map@10', 0.53), ('map@20', 0.533), ('precision@5', 0.162), ('precision@10', 0.082), ('precision@20', 0.043), ('recall@5', 0.808), ('recall@10', 0.819), ('recall@20', 0.857)])
Thu 27 Feb 2025 17:33:07 INFO  ['/opt/anaconda3/envs/recsys/lib/python3.10/site-packages/ipykernel_launcher.py', '--f=/Users/danielpenchev/Library/Jupyter/runtime/kernel-v32eaace8a7e8392ae2e21057923c8a41ccdf857ae.json']
Thu 27 Feb 2025 17:33:07 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 2020
state = INFO
reproducibility = True
data_path = ./data
checkpoint_dir = saved
show_progress = True
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 100
train_batch_size = 2048
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = False
metrics = ['Hit', 'MRR', 'MAP', 'Precision', 'Recall']
topk = [5, 10, 20]
valid_metric = MRR@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 3

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = ['train', 'extra', 'test']

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
mf_embedding_size = 64
mlp_embedding_size = 64
mlp_hidden_size = [128, 64]
dropout_prob = 0.1
mf_train = True
mlp_train = True
use_pretrain = False
mf_pretrain_path = None
mlp_pretrain_path = None
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.GENERAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cpu
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


Thu 27 Feb 2025 17:33:07 INFO  data
The number of users: 982
Average actions of users: 4.161060142711519
The number of items: 85
Average actions of items: 48.595238095238095
The number of inters: 4082
The sparsity of the dataset: 95.10962022283455%
Remain Fields: ['user_id', 'item_id']
Thu 27 Feb 2025 17:33:07 INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]
Thu 27 Feb 2025 17:33:07 INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
Thu 27 Feb 2025 17:33:07 INFO  NeuMF(
  (user_mf_embedding): Embedding(982, 64)
  (item_mf_embedding): Embedding(85, 64)
  (user_mlp_embedding): Embedding(982, 64)
  (item_mlp_embedding): Embedding(85, 64)
  (mlp_layers): MLPLayers(
    (mlp_layers): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=128, out_features=64, bias=True)
      (5): ReLU()
    )
  )
  (predict_layer): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
  (loss): BCEWithLogitsLoss()
)
Trainable parameters: 161473
Thu 27 Feb 2025 17:33:07 INFO  FLOPs: 24960.0
Thu 27 Feb 2025 17:33:08 INFO  epoch 0 training [time: 0.50s, train loss: 2.7714]
Thu 27 Feb 2025 17:33:08 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:08 INFO  epoch 1 training [time: 0.56s, train loss: 2.7665]
Thu 27 Feb 2025 17:33:08 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:09 INFO  epoch 2 training [time: 0.39s, train loss: 2.7550]
Thu 27 Feb 2025 17:33:09 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:09 INFO  epoch 3 training [time: 0.37s, train loss: 2.7259]
Thu 27 Feb 2025 17:33:09 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:09 INFO  epoch 4 training [time: 0.25s, train loss: 2.6791]
Thu 27 Feb 2025 17:33:09 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:10 INFO  epoch 5 training [time: 0.39s, train loss: 2.6079]
Thu 27 Feb 2025 17:33:10 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:10 INFO  epoch 6 training [time: 0.69s, train loss: 2.5014]
Thu 27 Feb 2025 17:33:10 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:11 INFO  epoch 7 training [time: 0.44s, train loss: 2.3835]
Thu 27 Feb 2025 17:33:11 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:11 INFO  epoch 8 training [time: 0.37s, train loss: 2.2069]
Thu 27 Feb 2025 17:33:11 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:12 INFO  epoch 9 training [time: 0.62s, train loss: 2.0033]
Thu 27 Feb 2025 17:33:12 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:12 INFO  epoch 10 training [time: 0.44s, train loss: 1.7077]
Thu 27 Feb 2025 17:33:12 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:13 INFO  epoch 11 training [time: 0.46s, train loss: 1.4823]
Thu 27 Feb 2025 17:33:13 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:13 INFO  epoch 12 training [time: 0.47s, train loss: 1.1231]
Thu 27 Feb 2025 17:33:13 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:14 INFO  epoch 13 training [time: 0.29s, train loss: 0.9101]
Thu 27 Feb 2025 17:33:14 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:14 INFO  epoch 14 training [time: 0.66s, train loss: 0.7599]
Thu 27 Feb 2025 17:33:14 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:15 INFO  epoch 15 training [time: 0.70s, train loss: 0.7086]
Thu 27 Feb 2025 17:33:15 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:16 INFO  epoch 16 training [time: 0.54s, train loss: 0.6301]
Thu 27 Feb 2025 17:33:16 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:16 INFO  epoch 17 training [time: 0.47s, train loss: 0.6109]
Thu 27 Feb 2025 17:33:16 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:17 INFO  epoch 18 training [time: 0.49s, train loss: 0.5601]
Thu 27 Feb 2025 17:33:17 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:17 INFO  epoch 19 training [time: 0.42s, train loss: 0.6461]
Thu 27 Feb 2025 17:33:17 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:18 INFO  epoch 20 training [time: 0.50s, train loss: 0.6115]
Thu 27 Feb 2025 17:33:18 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:18 INFO  epoch 21 training [time: 0.59s, train loss: 0.5151]
Thu 27 Feb 2025 17:33:18 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:19 INFO  epoch 22 training [time: 0.47s, train loss: 0.5103]
Thu 27 Feb 2025 17:33:19 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:19 INFO  epoch 23 training [time: 0.46s, train loss: 0.5217]
Thu 27 Feb 2025 17:33:19 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:20 INFO  epoch 24 training [time: 0.62s, train loss: 0.5715]
Thu 27 Feb 2025 17:33:20 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:20 INFO  epoch 25 training [time: 0.49s, train loss: 0.5707]
Thu 27 Feb 2025 17:33:20 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:21 INFO  epoch 26 training [time: 0.57s, train loss: 0.4733]
Thu 27 Feb 2025 17:33:21 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:22 INFO  epoch 27 training [time: 0.48s, train loss: 0.4733]
Thu 27 Feb 2025 17:33:22 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:22 INFO  epoch 28 training [time: 0.54s, train loss: 0.5117]
Thu 27 Feb 2025 17:33:22 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:23 INFO  epoch 29 training [time: 0.68s, train loss: 0.4992]
Thu 27 Feb 2025 17:33:23 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:23 INFO  epoch 30 training [time: 0.49s, train loss: 0.4419]
Thu 27 Feb 2025 17:33:23 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:24 INFO  epoch 31 training [time: 0.47s, train loss: 0.4351]
Thu 27 Feb 2025 17:33:24 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:24 INFO  epoch 32 training [time: 0.45s, train loss: 0.5159]
Thu 27 Feb 2025 17:33:24 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:25 INFO  epoch 33 training [time: 0.51s, train loss: 0.4743]
Thu 27 Feb 2025 17:33:25 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:25 INFO  epoch 34 training [time: 0.44s, train loss: 0.4512]
Thu 27 Feb 2025 17:33:25 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:26 INFO  epoch 35 training [time: 0.22s, train loss: 0.4699]
Thu 27 Feb 2025 17:33:26 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:26 INFO  epoch 36 training [time: 0.36s, train loss: 0.4484]
Thu 27 Feb 2025 17:33:26 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:27 INFO  epoch 37 training [time: 0.74s, train loss: 0.3731]
Thu 27 Feb 2025 17:33:27 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:27 INFO  epoch 38 training [time: 0.51s, train loss: 0.3858]
Thu 27 Feb 2025 17:33:27 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:28 INFO  epoch 39 training [time: 0.49s, train loss: 0.4375]
Thu 27 Feb 2025 17:33:28 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:28 INFO  epoch 40 training [time: 0.61s, train loss: 0.4255]
Thu 27 Feb 2025 17:33:28 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:29 INFO  epoch 41 training [time: 0.60s, train loss: 0.3588]
Thu 27 Feb 2025 17:33:29 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:29 INFO  epoch 42 training [time: 0.38s, train loss: 0.3695]
Thu 27 Feb 2025 17:33:29 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:30 INFO  epoch 43 training [time: 0.49s, train loss: 0.4123]
Thu 27 Feb 2025 17:33:30 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:30 INFO  epoch 44 training [time: 0.41s, train loss: 0.3613]
Thu 27 Feb 2025 17:33:30 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:31 INFO  epoch 45 training [time: 0.42s, train loss: 0.3716]
Thu 27 Feb 2025 17:33:31 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:31 INFO  epoch 46 training [time: 0.58s, train loss: 0.3771]
Thu 27 Feb 2025 17:33:31 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:32 INFO  epoch 47 training [time: 0.48s, train loss: 0.3284]
Thu 27 Feb 2025 17:33:32 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:33 INFO  epoch 48 training [time: 0.49s, train loss: 0.3863]
Thu 27 Feb 2025 17:33:33 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:33 INFO  epoch 49 training [time: 0.57s, train loss: 0.2951]
Thu 27 Feb 2025 17:33:33 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:34 INFO  epoch 50 training [time: 0.52s, train loss: 0.3428]
Thu 27 Feb 2025 17:33:34 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:34 INFO  epoch 51 training [time: 0.70s, train loss: 0.3375]
Thu 27 Feb 2025 17:33:34 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:35 INFO  epoch 52 training [time: 0.55s, train loss: 0.3802]
Thu 27 Feb 2025 17:33:35 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:36 INFO  epoch 53 training [time: 0.79s, train loss: 0.4414]
Thu 27 Feb 2025 17:33:36 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:36 INFO  epoch 54 training [time: 0.47s, train loss: 0.2979]
Thu 27 Feb 2025 17:33:36 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:37 INFO  epoch 55 training [time: 0.59s, train loss: 0.3058]
Thu 27 Feb 2025 17:33:37 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:37 INFO  epoch 56 training [time: 0.35s, train loss: 0.2747]
Thu 27 Feb 2025 17:33:37 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:38 INFO  epoch 57 training [time: 0.70s, train loss: 0.2740]
Thu 27 Feb 2025 17:33:38 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:39 INFO  epoch 58 training [time: 0.73s, train loss: 0.2740]
Thu 27 Feb 2025 17:33:39 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:40 INFO  epoch 59 training [time: 0.75s, train loss: 0.3246]
Thu 27 Feb 2025 17:33:40 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:40 INFO  epoch 60 training [time: 0.53s, train loss: 0.2273]
Thu 27 Feb 2025 17:33:40 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:41 INFO  epoch 61 training [time: 0.61s, train loss: 0.2644]
Thu 27 Feb 2025 17:33:41 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:41 INFO  epoch 62 training [time: 0.35s, train loss: 0.2821]
Thu 27 Feb 2025 17:33:41 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:42 INFO  epoch 63 training [time: 0.52s, train loss: 0.2621]
Thu 27 Feb 2025 17:33:42 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:42 INFO  epoch 64 training [time: 0.64s, train loss: 0.3063]
Thu 27 Feb 2025 17:33:42 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:43 INFO  epoch 65 training [time: 0.39s, train loss: 0.2529]
Thu 27 Feb 2025 17:33:43 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:43 INFO  epoch 66 training [time: 0.62s, train loss: 0.3235]
Thu 27 Feb 2025 17:33:43 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:44 INFO  epoch 67 training [time: 0.57s, train loss: 0.2782]
Thu 27 Feb 2025 17:33:44 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:44 INFO  epoch 68 training [time: 0.34s, train loss: 0.2574]
Thu 27 Feb 2025 17:33:44 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:45 INFO  epoch 69 training [time: 0.40s, train loss: 0.3221]
Thu 27 Feb 2025 17:33:45 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:45 INFO  epoch 70 training [time: 0.52s, train loss: 0.2674]
Thu 27 Feb 2025 17:33:45 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:46 INFO  epoch 71 training [time: 0.56s, train loss: 0.2330]
Thu 27 Feb 2025 17:33:46 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:46 INFO  epoch 72 training [time: 0.52s, train loss: 0.2973]
Thu 27 Feb 2025 17:33:46 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:47 INFO  epoch 73 training [time: 0.41s, train loss: 0.2330]
Thu 27 Feb 2025 17:33:47 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:47 INFO  epoch 74 training [time: 0.54s, train loss: 0.3018]
Thu 27 Feb 2025 17:33:48 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:48 INFO  epoch 75 training [time: 0.56s, train loss: 0.2292]
Thu 27 Feb 2025 17:33:48 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:49 INFO  epoch 76 training [time: 0.53s, train loss: 0.2247]
Thu 27 Feb 2025 17:33:49 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:49 INFO  epoch 77 training [time: 0.54s, train loss: 0.2228]
Thu 27 Feb 2025 17:33:49 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:50 INFO  epoch 78 training [time: 0.48s, train loss: 0.2280]
Thu 27 Feb 2025 17:33:50 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:50 INFO  epoch 79 training [time: 0.70s, train loss: 0.2230]
Thu 27 Feb 2025 17:33:50 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:51 INFO  epoch 80 training [time: 0.47s, train loss: 0.2196]
Thu 27 Feb 2025 17:33:51 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:51 INFO  epoch 81 training [time: 0.46s, train loss: 0.2646]
Thu 27 Feb 2025 17:33:51 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:52 INFO  epoch 82 training [time: 0.75s, train loss: 0.2293]
Thu 27 Feb 2025 17:33:52 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:53 INFO  epoch 83 training [time: 0.47s, train loss: 0.2035]
Thu 27 Feb 2025 17:33:53 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:53 INFO  epoch 84 training [time: 0.59s, train loss: 0.2048]
Thu 27 Feb 2025 17:33:53 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:54 INFO  epoch 85 training [time: 0.67s, train loss: 0.1948]
Thu 27 Feb 2025 17:33:54 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:54 INFO  epoch 86 training [time: 0.39s, train loss: 0.2547]
Thu 27 Feb 2025 17:33:55 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:55 INFO  epoch 87 training [time: 0.46s, train loss: 0.2289]
Thu 27 Feb 2025 17:33:55 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:56 INFO  epoch 88 training [time: 0.51s, train loss: 0.1940]
Thu 27 Feb 2025 17:33:56 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:56 INFO  epoch 89 training [time: 0.57s, train loss: 0.1972]
Thu 27 Feb 2025 17:33:56 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:57 INFO  epoch 90 training [time: 0.64s, train loss: 0.2085]
Thu 27 Feb 2025 17:33:57 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:58 INFO  epoch 91 training [time: 0.78s, train loss: 0.2234]
Thu 27 Feb 2025 17:33:58 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:58 INFO  epoch 92 training [time: 0.43s, train loss: 0.1669]
Thu 27 Feb 2025 17:33:58 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:59 INFO  epoch 93 training [time: 0.44s, train loss: 0.1815]
Thu 27 Feb 2025 17:33:59 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:33:59 INFO  epoch 94 training [time: 0.61s, train loss: 0.2144]
Thu 27 Feb 2025 17:33:59 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:34:00 INFO  epoch 95 training [time: 0.86s, train loss: 0.1817]
Thu 27 Feb 2025 17:34:00 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:34:01 INFO  epoch 96 training [time: 0.44s, train loss: 0.1826]
Thu 27 Feb 2025 17:34:01 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:34:01 INFO  epoch 97 training [time: 0.31s, train loss: 0.1822]
Thu 27 Feb 2025 17:34:01 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:34:02 INFO  epoch 98 training [time: 0.64s, train loss: 0.1919]
Thu 27 Feb 2025 17:34:02 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:34:02 INFO  epoch 99 training [time: 0.52s, train loss: 0.1843]
Thu 27 Feb 2025 17:34:02 INFO  Saving current: saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:34:02 INFO  Loading model structure and parameters from saved/NeuMF-Feb-27-2025_17-33-07.pth
Thu 27 Feb 2025 17:34:04 INFO  The running environment of this training is as follows:
+-------------+----------------+
| Environment |     Usage      |
+=============+================+
| CPU         |    23.80 %     |
+-------------+----------------+
| GPU         |   0.0 / 0.0    |
+-------------+----------------+
| Memory      | 0.54 G/24.00 G |
+-------------+----------------+
Thu 27 Feb 2025 17:34:04 INFO  best valid : None
Thu 27 Feb 2025 17:34:04 INFO  test result: OrderedDict([('hit@5', 0.808), ('hit@10', 0.819), ('hit@20', 0.857), ('mrr@5', 0.529), ('mrr@10', 0.53), ('mrr@20', 0.533), ('map@5', 0.529), ('map@10', 0.53), ('map@20', 0.533), ('precision@5', 0.162), ('precision@10', 0.082), ('precision@20', 0.043), ('recall@5', 0.808), ('recall@10', 0.819), ('recall@20', 0.857)])
